{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood of Encrypted Neural Networks\n",
    "\n",
    "This tutorial is optional, and can be skipped without loss of continuity.\n",
    "\n",
    "In this tutorial, we'll take a look at how CrypTen performs inference with an encrypted neural network on encrypted data. We'll see how the data remains encrypted through all the operations, and yet is able to obtain accurate results after the computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "\n",
    "crypten.init() \n",
    "\n",
    "# Ignore warnings\n",
    "import warnings; \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Keep track of all created temporary files so that we can clean up at the end\n",
    "temp_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Linear Layer\n",
    "We'll start by examining how a single Linear layer works in CrypTen. We'll instantiate a torch Linear layer, convert to CrypTen layer, encrypt it, and step through some toy data with it. As in earlier tutorials, we'll assume Alice has the rank 0 process and Bob has the rank 1 process. We'll also assume Alice has the layer and Bob has the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ALICE and BOB src values\n",
    "ALICE = 0\n",
    "BOB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaintext Weights: Parameter containing:\n",
      "tensor([[ 0.3941, -0.4677, -0.4373, -0.4780],\n",
      "        [-0.1308,  0.2255, -0.2533, -0.0447]], requires_grad=True)\n",
      "Plaintext Bias: Parameter containing:\n",
      "tensor([-0.0189, -0.0790], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate single Linear layer\n",
    "layer_linear = nn.Linear(4, 2)\n",
    "\n",
    "# The weights and the bias are initialized to small random values\n",
    "print(\"Plaintext Weights:\", layer_linear._parameters['weight'])\n",
    "print(\"Plaintext Bias:\", layer_linear._parameters['bias'])\n",
    "\n",
    "# Save the plaintext layer\n",
    "layer_linear_file = \"/tmp/tutorial5_layer_alice1.pth\"\n",
    "crypten.save(layer_linear, layer_linear_file)\n",
    "temp_files.append(layer_linear_file) \n",
    "\n",
    "# Generate some toy data\n",
    "features = 4\n",
    "examples = 3\n",
    "toy_data = torch.rand(examples, features)\n",
    "\n",
    "# Save the plaintext toy data\n",
    "toy_data_file = \"/tmp/tutorial5_data_bob1.pth\"\n",
    "crypten.save(toy_data, toy_data_file)\n",
    "temp_files.append(toy_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1 Encrypted data:\n",
      "tensor([[ 3678700102179911956, -2909050226621709958,  2811555063257232306,\n",
      "         -4928920039109496584],\n",
      "        [-5461804489811683468,  2507827404725734825, -1277464813380873939,\n",
      "          4588017341358575108],\n",
      "        [ 5802222930185798095, -3989271002924527689,  8211929695094444304,\n",
      "          3369799817375354834]])\n",
      "Rank: 0 Encrypted data:\n",
      "tensor([[-3678700102179867573,  2909050226621752754, -2811555063257171909,\n",
      "          4928920039109526119],\n",
      "        [ 5461804489811700428, -2507827404725680380,  1277464813380918463,\n",
      "         -4588017341358574103],\n",
      "        [-5802222930185753362,  3989271002924574164, -8211929695094428483,\n",
      "         -3369799817375320623]])\n",
      "\n",
      "\n",
      "Rank: 0 Weights Encrypted: True\n",
      "Rank: 1 Weights Encrypted: True\n",
      "Rank: 0 Bias Encrypted: True\n",
      "Rank: 1 Bias Encrypted: True\n",
      "Rank: 0 Encrypted Weights:\n",
      "tensor([[-7136390616833108556,  7942355782122678145,  8415727642310264347,\n",
      "         -1381325403809972801],\n",
      "        [ 4877797614814754119,  1687438649584220050,  1823703675889772873,\n",
      "          -196270047732166246]])Rank: 1 Encrypted Weights:\n",
      "tensor([[ 7136390616833134382, -7942355782122708798, -8415727642310293005,\n",
      "          1381325403809941474],\n",
      "        [-4877797614814762691, -1687438649584205269, -1823703675889789473,\n",
      "           196270047732163316]])\n",
      "\n",
      "Rank: 0 Encrypted Bias: tensor([ 8202471088837329805, -6153076127978832706])\n",
      "Rank: 1 Encrypted Bias: tensor([-8202471088837331045,  6153076127978827530])\n",
      "\n",
      "\n",
      "Rank: 1 Encrypted result:\n",
      "tensor([[-8202585217428429056,  6153138975805248388],\n",
      "        [-8202582059863762311,  6153121341663252941],\n",
      "        [-8202360804050122020,  6152987077684976402]])\n",
      "Rank: 0 Encrypted result:\n",
      "tensor([[ 8202585217428384761, -6153138975805266336],\n",
      "        [ 8202582059863722339, -6153121341663259378],\n",
      "        [ 8202360804050093399, -6152987077684982484]])\n",
      "\n",
      "\n",
      "Decrypted result:\n",
      " tensor([[-0.6759, -0.2739],\n",
      "        [-0.6099, -0.0982],\n",
      "        [-0.4367, -0.0928]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def forward_single_encrypted_layer():\n",
    "    rank = comm.get().get_rank()\n",
    "    \n",
    "    # Load and encrypt the layer\n",
    "    layer = crypten.load(layer_linear_file, dummy_model=nn.Linear(4, 2), src=ALICE)\n",
    "    layer_enc = crypten.nn.from_pytorch(layer, dummy_input=torch.empty((1,4)))\n",
    "    layer_enc.encrypt(src=ALICE)\n",
    "    \n",
    "    # Load and encrypt data\n",
    "    data = crypten.load(toy_data_file, src=BOB)    \n",
    "    data_enc = crypten.cryptensor(data, src=BOB) \n",
    "    \n",
    "    # Examine the shares inside the encrypted data\n",
    "    print(\"Rank: {} Encrypted data:\\n{}\\n\".format(rank, data_enc.share))\n",
    "\n",
    "    # Examine the weights and the bias of the linear layer again\n",
    "    # First, we'll see that weights and bias have become encrypted tensors\n",
    "    print(\"Rank: {} Weights Encrypted: {}\".format(rank, crypten.is_encrypted_tensor(layer_enc._parameters['weight'])))\n",
    "    print(\"Rank: {} Bias Encrypted: {}\".format(rank, crypten.is_encrypted_tensor(layer_enc._parameters['bias'])))\n",
    "    # Now let's look at the tensor values\n",
    "    print(\"Rank: {} Encrypted Weights:\\n{}\".format(rank, layer_enc._parameters['weight'].share))\n",
    "    print(\"Rank: {} Encrypted Bias: {}\".format(rank, layer_enc._parameters['bias'].share))\n",
    "    print()\n",
    "\n",
    "    # Apply the encrypted layer: encrypted linear transformation \n",
    "    result_enc = layer_enc.forward(data_enc)\n",
    "    # Examine the resulting shares of both parties\n",
    "    print(\"Rank: {} Encrypted result:\\n{}\\n\".format(rank, result_enc.share))\n",
    "    \n",
    "    # Decrypt the result:\n",
    "    result_plaintext = result_enc.get_plain_text()\n",
    "    # Since both parties have the same decrypted values, print only rank 0 values for readability\n",
    "    if rank == 0:\n",
    "        print(\"Decrypted result:\\n\", result_plaintext)\n",
    "        \n",
    "forward_single_encrypted_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the application of the encrypted linear layer on the encrypted data produces an encrypted result, which we can then decrypt to get the values in plaintext.\n",
    "\n",
    "Let's look at a second linear transformation, to give a flavor of how accuracy is preserved even when the data and the layer are encrypted. We'll look at a uniform scaling transformation, in which all tensor elements are multiplied by the same scalar factor. Again, we'll assume Alice has the layer and the rank 0 process, and Bob has the data and the rank 1 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a linear layer with random weights\n",
    "layer_scale = nn.Linear(3, 3)\n",
    "\n",
    "# Construct a uniform scaling matrix: we'll scale by factor 5\n",
    "factor = 5\n",
    "layer_scale._parameters['weight'] = torch.eye(3)*factor\n",
    "layer_scale._parameters['bias'] = torch.zeros_like(layer_scale._parameters['bias'])\n",
    "\n",
    "# Save the plaintext layer\n",
    "layer_scale_file = \"/tmp/tutorial5_layer_alice2.pth\"\n",
    "crypten.save(layer_scale, layer_scale_file)\n",
    "temp_files.append(layer_scale_file)\n",
    "\n",
    "# Construct some toy data\n",
    "features = 3\n",
    "examples = 2\n",
    "toy_data = torch.ones(examples, features)\n",
    "\n",
    "# Save the plaintext toy data\n",
    "toy_data_file = \"/tmp/tutorial5_data_bob2.pth\"\n",
    "crypten.save(toy_data, toy_data_file)\n",
    "temp_files.append(toy_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1 Encrypted data:\n",
      "tensor([[ 2380050498413143599,  1847879776682561747, -8964731574872462529],\n",
      "        [-3234938927652527609, -4572902848872977324,  2461944650304271428]])\n",
      "Rank: 0 Encrypted data:\n",
      "tensor([[-2380050498413078063, -1847879776682496211,  8964731574872528065],\n",
      "        [ 3234938927652593145,  4572902848873042860, -2461944650304205892]])\n",
      "\n",
      "\n",
      "Rank: 0 Encrypted Weights:\n",
      "tensor([[-8878088872997265406, -6954459607753448167,  -671476567678641743],\n",
      "        [ 6019696710699457951,  8827473402821762442,  5194317374079125986],\n",
      "        [-7659511891485929821, -2295374719619823579, -2090928637492781555]])Rank: 1 Encrypted Weights:\n",
      "tensor([[ 8878088872997593086,  6954459607753448167,   671476567678641743],\n",
      "        [-6019696710699457951, -8827473402821434762, -5194317374079125986],\n",
      "        [ 7659511891485929821,  2295374719619823579,  2090928637493109235]])\n",
      "\n",
      "Rank: 0 Encrypted Bias:\n",
      "tensor([-1707544518694688470, -4483419037710543608, -7445515754401636293])Rank: 1 Encrypted Bias:\n",
      "tensor([1707544518694688470, 4483419037710543608, 7445515754401636293])\n",
      "\n",
      "Rank: 0 Encrypted result:\n",
      " tensor([[-1707680512414947679, -4483371767269072539, -7445574442514947148],\n",
      "        [-1707635658741573948, -4483507577637410955, -7445601862880865387]])\n",
      "Rank: 1 Encrypted result:\n",
      " tensor([[1707680512415275359, 4483371767269400219, 7445574442515274828],\n",
      "        [1707635658741901628, 4483507577637738635, 7445601862881193067]])\n",
      "\n",
      "\n",
      "Plaintext result:\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def forward_scaling_layer():\n",
    "    rank = comm.get().get_rank()\n",
    "    \n",
    "    # Load and encrypt the layer\n",
    "    layer = crypten.load(layer_scale_file, dummy_model=nn.Linear(3, 3), src=ALICE)\n",
    "    layer_enc = crypten.nn.from_pytorch(layer, dummy_input=torch.empty((1,3)))\n",
    "    layer_enc.encrypt(src=ALICE)\n",
    "    \n",
    "    # Load and encrypt data\n",
    "    data = crypten.load(toy_data_file, src=BOB)    \n",
    "    data_enc = crypten.cryptensor(data, src=BOB)     \n",
    "    \n",
    "    # Examine the encrypted data\n",
    "    print(\"Rank: {} Encrypted data:\\n{}\\n\".format(rank, data_enc.share))\n",
    "        \n",
    "    # Examine the encrypted layer\n",
    "    print(\"Rank: {} Encrypted Weights:\\n{}\".format(rank, layer_enc._parameters['weight'].share))\n",
    "    print(\"Rank: {} Encrypted Bias:\\n{}\".format(rank, layer_enc._parameters['bias'].share))\n",
    "\n",
    "    # Apply the encrypted scaling transformation\n",
    "    result_enc = layer_enc.forward(data_enc)\n",
    "    \n",
    "    # Examine the encrypted results\n",
    "    print(\"Rank: {} Encrypted result:\\n {}\\n\".format(rank, result_enc.share))\n",
    "\n",
    "    # Decrypt the result:\n",
    "    result_plaintext = result_enc.get_plain_text()\n",
    "    # Since both parties have the same decrypted values, print only rank 0 for readability\n",
    "    if rank == 0:\n",
    "        print(\"Plaintext result:\\n{}\".format(result_plaintext))\n",
    "        \n",
    "z = forward_scaling_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plaintext tensor is correctly scaled, even though we applied the encrypted transformation on the encrypted input! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Neural Networks\n",
    "Let's now look at how the encrypted input moves through an encrypted multi-layer neural network. \n",
    "\n",
    "For ease of explanation, we'll first step through a network with only two linear layers and ReLU activations. Again, we'll assume Alice has a network and Bob has some data, and they wish to run encrypted inference. \n",
    "\n",
    "To simulate this, we'll once again generate some toy data and train Alice's network on it. Then we'll encrypt Alice's network, Bob's data, and step through every layer in the network with the encrypted data. Through this, we'll see how the computations get applied although the network and the data are encrypted.\n",
    "\n",
    "### Setup\n",
    "As in Tutorial 3, we will first generate 1000 ground truth samples using 50 features and a randomly generated hyperplane to separate positive and negative examples. We will then modify the labels so that they are all non-negative. Finally, we will split the data so that the first 900 samples belong to Alice and the last 100 samples belong to Bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "features = 50\n",
    "examples = 1000\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Generate toy data and separating hyperplane\n",
    "data = torch.randn(examples, features)\n",
    "w_true = torch.randn(1, features)\n",
    "b_true = torch.randn(1)\n",
    "labels = w_true.matmul(data.t()).add(b_true).sign()\n",
    "\n",
    "# Change labels to non-negative values\n",
    "labels_nn = torch.where(labels==-1, torch.zeros(labels.size()), labels)\n",
    "labels_nn = labels_nn.squeeze().long()\n",
    "\n",
    "# Split data into Alice's and Bob's portions:\n",
    "data_alice, labels_alice = data[:900], labels_nn[:900]\n",
    "data_bob, labels_bob = data[900:], labels_nn[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Alice's network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Loss: 0.2470429241657257\n",
      "Epoch 199 Loss: 0.08965438604354858\n",
      "Epoch 299 Loss: 0.05166155472397804\n",
      "Epoch 399 Loss: 0.03510778397321701\n",
      "Epoch 499 Loss: 0.026072446256875992\n"
     ]
    }
   ],
   "source": [
    "# Train and save Alice's network\n",
    "model = AliceNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(500):  \n",
    "    #forward pass: compute prediction\n",
    "    output = model(data_alice)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = criterion(output, labels_alice)\n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch\", i, \"Loss:\", loss.item())\n",
    "    \n",
    "    #zero gradients for learnable parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #backward pass: compute gradient with respect to model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    #update model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "sample_trained_model_file = '/tmp/tutorial5_alice_model.pth'\n",
    "torch.save(model, sample_trained_model_file)\n",
    "temp_files.append(sample_trained_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping through a Multi-layer Network\n",
    "\n",
    "Let's now look at what happens when we load the network Alice's has trained and encrypt it. First, we'll look at how the network structure changes when we convert it from a PyTorch network to CrypTen network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 5 \tModule: <crypten.nn.module.Linear object at 0x7fde8808f590>\n",
      "Name: 6 \tModule: <crypten.nn.module.ReLU object at 0x7fdee8479650>\n",
      "Name: output \tModule: <crypten.nn.module.Linear object at 0x7fdee84798d0>\n"
     ]
    }
   ],
   "source": [
    "# Load the trained network to Alice\n",
    "model_plaintext = crypten.load(sample_trained_model_file, dummy_model=AliceNet(), src=ALICE)\n",
    "\n",
    "# Convert the trained network to CrypTen network \n",
    "private_model = crypten.nn.from_pytorch(model_plaintext, dummy_input=torch.empty((1, 50)))\n",
    "# Encrypt the network\n",
    "private_model.encrypt(src=ALICE)\n",
    "\n",
    "# Examine the structure of the encrypted CrypTen network\n",
    "for name, curr_module in private_model._modules.items():\n",
    "    print(\"Name:\", name, \"\\tModule:\", curr_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the encrypted network has 3 modules, named '5', '6' and 'output', denoting the first Linear layer, the ReLU activation, and the second Linear layer respectively. These modules are encrypted just as the layers in the previous section were. \n",
    "\n",
    "Now let's encrypt Bob's data, and step it through each encrypted module. For readability, we will use only 3 examples from Bob's data to illustrate the inference. Note how Bob's data remains encrypted after each individual layer's computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Select only the first three examples in Bob's data for readability\n",
    "data = data_bob[:3]\n",
    "sample_data_bob_file = '/tmp/tutorial5_data_bob3.pth'\n",
    "torch.save(data, sample_data_bob_file)\n",
    "temp_files.append(sample_data_bob_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0 First Linear Layer: Output Encrypted: True\n",
      "Rank: 1 First Linear Layer: Output Encrypted: True\n",
      "\n",
      "\n",
      "Rank: 1 Shares after First Linear Layer:tensor([[-2224392792640713959,  6083933912752127281, -2049841560836145201,\n",
      "         -3804603540994371973, -1282845883776053085,  3271623645745577736,\n",
      "         -2962090725151140639,  1405323579613973433,  7555130549438824985,\n",
      "         -6931999527211605420, -8285657089197970503,  1314901661728837421,\n",
      "         -5620760993017059357,  7475902553670408973, -8756112654425768089,\n",
      "         -2233694146531187381, -4242376502263878867,  1417352514974486780,\n",
      "          4437703783822774923,  4681976352489113695],\n",
      "        [-2224550209238588380,  6083918444126157129, -2049776429354933648,\n",
      "         -3804578160550366625, -1282927694982073609,  3271677400736939913,\n",
      "         -2961963807683856724,  1405274710644603910,  7555073749451748894,\n",
      "         -6931788212312501229, -8285572078619406303,  1314791283283139625,\n",
      "         -5620638356681602645,  7475843603576212719, -8756126194168971428,\n",
      "         -2233688872976761371, -4242302115963881648,  1417192283611769083,\n",
      "          4437950245077647594,  4682001309747163086],\n",
      "        [-2224348708357784712,  6083859552472633565, -2049858924975969239,\n",
      "         -3804587074568688697, -1282787652360186231,  3271568509635994974,\n",
      "         -2962112846438480375,  1405329343349963250,  7555126668357732846,\n",
      "         -6931953462975216394, -8285496842283165357,  1315050013463309512,\n",
      "         -5620583930093613714,  7475780788630339443, -8756054320443037133,\n",
      "         -2233725345126081893, -4242284201149072180,  1417139112334633121,\n",
      "          4437738218313842272,  4682068833561702634]])\n",
      "Rank: 0 Shares after First Linear Layer:tensor([[ 2224392792640712635, -6083933912752131848,  2049841560836140284,\n",
      "          3804603540994390581,  1282845883776069674, -3271623645745624425,\n",
      "          2962090725151151037, -1405323579613950926, -7555130549438801336,\n",
      "          6931999527211609248,  8285657089197980712, -1314901661728825420,\n",
      "          5620760993017157091, -7475902553670456706,  8756112654425845922,\n",
      "          2233694146531206785,  4242376502263921596, -1417352514974491214,\n",
      "         -4437703783822840202, -4681976352489130343],\n",
      "        [ 2224550209238656682, -6083918444126206266,  2049776429355008842,\n",
      "          3804578160550370910,  1282927694982085012, -3271677400736993072,\n",
      "          2961963807683891431, -1405274710644528282, -7555073749451734945,\n",
      "          6931788212312433458,  8285572078619365507, -1314791283283100461,\n",
      "          5620638356681518705, -7475843603576218668,  8756126194169039663,\n",
      "          2233688872976726557,  4242302115963897195, -1417192283611801199,\n",
      "         -4437950245077673955, -4682001309747045903],\n",
      "        [ 2224348708357782549, -6083859552472594134,  2049858924976121379,\n",
      "          3804587074568778018,  1282787652360163677, -3271568509636000019,\n",
      "          2962112846438454913, -1405329343349879256, -7555126668357748839,\n",
      "          6931953462975291773,  8285496842283249502, -1315050013463336434,\n",
      "          5620583930093511517, -7475780788630383083,  8756054320442934344,\n",
      "          2233725345125991426,  4242284201148972303, -1417139112334640288,\n",
      "         -4437738218313779661, -4682068833561722344]])\n",
      "\n",
      "\n",
      "Rank: 0 ReLU:\n",
      " Output Encrypted: True\n",
      "Rank: 1 ReLU:\n",
      " Output Encrypted: True\n",
      "\n",
      "\n",
      "Rank: 0 Shares after ReLU: tensor([[-106198790535108,  -72345043586013,   47641237258336,  -39447367007257,\n",
      "           49155559708318,  -71473481619236,  -36615771705985,  -88760192356805,\n",
      "           24294037431996,  -94001571968647,   -6523827979624,   33747665313578,\n",
      "           99327749467752,   66768867398522,   53139065439496,  -64371042184968,\n",
      "          -97302498292721,  -91658770158093,  -91267863030673,   66074624028732],\n",
      "        [ -19226602249300,   58690398107657,  -25368503243611,   79845516599356,\n",
      "           33033242160747,   32445524760708, -129908702889031,  -64182620529062,\n",
      "           99329223139713,  -57337390282401,   32228494048532,  115727051889970,\n",
      "         -111033470866967, -113156911146731,   83966889013662,  -47312924252867,\n",
      "          -44488226129618, -100193050142771,  137358961011136,    4145775873482],\n",
      "        [  30368182328275,  -70955878215471,  118124420389329,   95498129867560,\n",
      "          -73023655975332, -122227308446142,  -20216750742833, -127196110182056,\n",
      "           24044492607233, -135880100589087,   79027622484669,  112771678167125,\n",
      "          119148225837964,  -62192369085038,   -1139567781002,  -83531222475492,\n",
      "           75533814950537,  -15912179290573,  115940893054185, -107636263975076]])\n",
      "Rank: 1 Shares after ReLU: tensor([[ 106198790535108,   72345043586013,  -47641237258336,   39447367025865,\n",
      "          -49155559691729,   71473481619236,   36615771716383,   88760192379312,\n",
      "          -24294037408347,   94001571972475,    6523827989833,  -33747665301577,\n",
      "          -99327749370018,  -66768867398522,  -53139065361663,   64371042204372,\n",
      "           97302498335450,   91658770158093,   91267863030673,  -66074624028732],\n",
      "        [  19226602317602,  -58690398107657,   25368503318805,  -79845516595071,\n",
      "          -33033242149344,  -32445524760708,  129908702923738,   64182620604690,\n",
      "          -99329223125764,   57337390282401,  -32228494048532, -115727051850806,\n",
      "          111033470866967,  113156911146731,  -83966888945427,   47312924252867,\n",
      "           44488226145165,  100193050142771, -137358961011136,   -4145775756299],\n",
      "        [ -30368182328275,   70955878254902, -118124420237189,  -95498129778239,\n",
      "           73023655975332,  122227308446142,   20216750742833,  127196110266050,\n",
      "          -24044492607233,  135880100664466,  -79027622400524, -112771678167125,\n",
      "         -119148225837964,   62192369085038,    1139567781002,   83531222475492,\n",
      "          -75533814950537,   15912179290573, -115940892991574,  107636263975076]])\n",
      "\n",
      "\n",
      "Rank: 1 Second Linear layer:\n",
      " Output Encrypted: True\n",
      "Rank: 0 Second Linear layer:\n",
      " Output Encrypted: True\n",
      "\n",
      "\n",
      "Rank: 0 Shares after Second Linear layer:tensor([[  347494528038419320, -4623351898367509110],\n",
      "        [  347379605252333935, -4623164582803840656],\n",
      "        [  347377045794527134, -4623078755860395957]])\n",
      "Rank: 1 Shares after Second Linear layer:tensor([[-347494528038592019, 4623351898367678603],\n",
      "        [-347379605252193909, 4623164582803754703],\n",
      "        [-347377045794241001, 4623078755860172583]])\n",
      "\n",
      "\n",
      "Decrypted output:\n",
      " Output Encrypted: False\n",
      "Tensors:\n",
      " tensor([[-2.6352,  2.5863],\n",
      "        [ 2.1366, -1.3115],\n",
      "        [ 4.3660, -3.4084]])\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def step_through_two_layers():    \n",
    "    rank = comm.get().get_rank()\n",
    "\n",
    "    # Load and encrypt the network\n",
    "    model = crypten.load(sample_trained_model_file, dummy_model=AliceNet(), src=ALICE)\n",
    "    private_model = crypten.nn.from_pytorch(model, dummy_input=torch.empty((1, 50)))\n",
    "    private_model.encrypt(src=ALICE)\n",
    "\n",
    "    # Load and encrypt the data\n",
    "    data = crypten.load(sample_data_bob_file, src=BOB)\n",
    "    data_enc = crypten.cryptensor(data, src=BOB)\n",
    "\n",
    "    # Forward through the first layer\n",
    "    out_enc = private_model._modules['5'].forward(data_enc)\n",
    "    print(\"Rank: {} First Linear Layer: Output Encrypted: {}\\n\".format(rank, crypten.is_encrypted_tensor(out_enc)))\n",
    "    print(\"Rank: {} Shares after First Linear Layer:{}\\n\".format(rank, out_enc.share))\n",
    "\n",
    "    # Apply ReLU activation\n",
    "    out_enc = private_model._modules['6'].forward(out_enc)\n",
    "    print(\"Rank: {} ReLU:\\n Output Encrypted: {}\\n\".format(rank, crypten.is_encrypted_tensor(out_enc)))\n",
    "    print(\"Rank: {} Shares after ReLU: {}\\n\".format(rank, out_enc.share))\n",
    "\n",
    "    # Forward through the second Linear layer\n",
    "    out_enc = private_model._modules['output'].forward(out_enc)\n",
    "    print(\"Rank: {} Second Linear layer:\\n Output Encrypted: {}\\n\".format(rank, crypten.is_encrypted_tensor(out_enc))), \n",
    "    print(\"Rank: {} Shares after Second Linear layer:{}\\n\".format(rank, out_enc.share))\n",
    "\n",
    "    # Decrypt the output\n",
    "    out_dec = out_enc.get_plain_text()\n",
    "    # Since both parties have same decrypted results, only print the rank 0 output\n",
    "    if rank == 0:\n",
    "        print(\"Decrypted output:\\n Output Encrypted:\", crypten.is_encrypted_tensor(out_dec))\n",
    "        print(\"Tensors:\\n\", out_dec)\n",
    "    \n",
    "z = step_through_two_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we emphasize that the output of each layer is an encrypted tensor. Only after the final call to `get_plain_text` do we get the plaintext tensor.\n",
    "\n",
    "### From PyTorch to CrypTen: Structural Changes in Network Architecture \n",
    "\n",
    "We have used a simple two-layer network in the above example, but the same ideas apply to more complex networks and operations. However, in more complex networks, there may not always be a one-to-one mapping between the PyTorch layers and the CrypTen layers. As an example, we'll take a typical network used to classify digits in MNIST data, and look at what happens to its structure when encrypted. (As we only wish to illustrate the structural changes in layers, we will not train this network on data; we will just use it with its randomly initialized weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 24 \tModule: <crypten.nn.module.Conv2d object at 0x7fdec87d3990>\n",
      "Name: 25 \tModule: <crypten.nn.module._BatchNorm object at 0x7fdec87d3910>\n",
      "Name: 26 \tModule: <crypten.nn.module.ReLU object at 0x7fdec87d3cd0>\n",
      "Name: 27 \tModule: <crypten.nn.module._ConstantPad object at 0x7fdec87d3290>\n",
      "Name: 28 \tModule: <crypten.nn.module.AvgPool2d object at 0x7fdec87d3150>\n",
      "Name: 29 \tModule: <crypten.nn.module.Conv2d object at 0x7fded8535890>\n",
      "Name: 30 \tModule: <crypten.nn.module._BatchNorm object at 0x7fdec87d3350>\n",
      "Name: 31 \tModule: <crypten.nn.module.ReLU object at 0x7fdec87d32d0>\n",
      "Name: 32 \tModule: <crypten.nn.module._ConstantPad object at 0x7fdec87d3810>\n",
      "Name: 33 \tModule: <crypten.nn.module.AvgPool2d object at 0x7fdec87d3210>\n",
      "Name: 34 \tModule: <crypten.nn.module.Constant object at 0x7fdec87d3590>\n",
      "Name: 35 \tModule: <crypten.nn.module.Shape object at 0x7fdec87d3250>\n",
      "Name: 36 \tModule: <crypten.nn.module.Gather object at 0x7fdec87d3850>\n",
      "Name: 37 \tModule: <crypten.nn.module.Constant object at 0x7fdec87d3f90>\n",
      "Name: 38 \tModule: <crypten.nn.module.Unsqueeze object at 0x7fdec87d38d0>\n",
      "Name: 39 \tModule: <crypten.nn.module.Unsqueeze object at 0x7fdec87d3a50>\n",
      "Name: 40 \tModule: <crypten.nn.module.Concat object at 0x7fdec87d3a90>\n",
      "Name: 41 \tModule: <crypten.nn.module.Reshape object at 0x7fdec87d3e50>\n",
      "Name: 42 \tModule: <crypten.nn.module.Linear object at 0x7fdec87d3c50>\n",
      "Name: 43 \tModule: <crypten.nn.module.Unsqueeze object at 0x7fdec87d0510>\n",
      "Name: 44 \tModule: <crypten.nn.module._BatchNorm object at 0x7fdec87d0350>\n",
      "Name: 45 \tModule: <crypten.nn.module.Squeeze object at 0x7fdec87d01d0>\n",
      "Name: 46 \tModule: <crypten.nn.module.ReLU object at 0x7fdec87d0310>\n",
      "Name: output \tModule: <crypten.nn.module.Linear object at 0x7fdec87d07d0>\n"
     ]
    }
   ],
   "source": [
    "# Define Alice's network\n",
    "class AliceNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(100)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.batchnorm3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = AliceNet2()\n",
    "\n",
    "# Let's encrypt the complex network. \n",
    "# Create dummy input of the correct input shape for the model\n",
    "dummy_input = torch.empty((1, 1, 28, 28))\n",
    "\n",
    "# Encrypt the network\n",
    "private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "private_model.encrypt(src=ALICE)\n",
    "\n",
    "# Examine the structure of the encrypted network\n",
    "for name, curr_module in private_model._modules.items():\n",
    "    print(\"Name:\", name, \"\\tModule:\", curr_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the CrypTen network has split the PyTorch network into several additional layers. Each PyTorch operation may correspond to one or more operations in CrypTen. Nevertheless, the same ideas apply when forwarding the data through the CrypTen equivalent of each PyTorch operation.  \n",
    "\n",
    "Before exiting this tutorial, please clean up the files generated using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for fn in temp_files:\n",
    "    if os.path.exists(fn): os.remove(fn)"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "511641209674539"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139530",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "417315762230172",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 5 -- Under the hood of Encrypted Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
