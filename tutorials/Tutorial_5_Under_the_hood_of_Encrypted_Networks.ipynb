{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood of Encrypted Neural Networks\n",
    "\n",
    "This tutorial is optional, and can be skipped without loss of continuity.\n",
    "\n",
    "In this tutorial, we'll take a look at how CrypTen performs inference with an encrypted neural network on encrypted data. We'll see how the data remains encrypted through all the operations, and yet is able to obtain accurate results after the computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "\n",
    "crypten.init() \n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings; \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Keep track of all created temporary files so that we can clean up at the end\n",
    "temp_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Linear Layer\n",
    "We'll start by examining how a single Linear layer works in CrypTen. We'll instantiate a torch Linear layer, convert to CrypTen layer, encrypt it, and step through some toy data with it. As in earlier tutorials, we'll assume Alice has the rank 0 process and Bob has the rank 1 process. We'll also assume Alice has the layer and Bob has the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ALICE and BOB src values\n",
    "ALICE = 0\n",
    "BOB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaintext Weights:\n",
      "\n",
      " Parameter containing:\n",
      "tensor([[-0.0337, -0.3834,  0.1899,  0.1072],\n",
      "        [-0.2576,  0.3539, -0.1368, -0.0071]], requires_grad=True)\n",
      "\n",
      "Plaintext Bias:\n",
      "\n",
      " Parameter containing:\n",
      "tensor([0.2827, 0.1387], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate single Linear layer\n",
    "layer_linear = nn.Linear(4, 2)\n",
    "\n",
    "# The weights and the bias are initialized to small random values\n",
    "print(\"Plaintext Weights:\\n\\n\", layer_linear._parameters['weight'])\n",
    "print(\"\\nPlaintext Bias:\\n\\n\", layer_linear._parameters['bias'])\n",
    "\n",
    "# Save the plaintext layer\n",
    "layer_linear_file = \"/tmp/tutorial5_layer_alice1.pth\"\n",
    "crypten.save(layer_linear, layer_linear_file)\n",
    "temp_files.append(layer_linear_file) \n",
    "\n",
    "# Generate some toy data\n",
    "features = 4\n",
    "examples = 3\n",
    "toy_data = torch.rand(examples, features)\n",
    "\n",
    "# Save the plaintext toy data\n",
    "toy_data_file = \"/tmp/tutorial5_data_bob1.pth\"\n",
    "crypten.save(toy_data, toy_data_file)\n",
    "temp_files.append(toy_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " tensor([[ 3701425873623077417,  3131436260899300567,  4761263973767768107,\n",
      "         -6049547285147554296],\n",
      "        [-2785416995674118958, -8231716559199556861,   895688907941851881,\n",
      "          1789843895485185368]])\n",
      "Bias:\n",
      " tensor([ -907626009965645231, -5503748647191607612]) \n",
      "\n",
      "Get attribute forward\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[-6145106062772284459,  3780024118121113519, -7689609490617789212,\n",
      "          5861391885217626753],\n",
      "        [ 7295450244422357022, -5533276316057545045,  3327749906166077255,\n",
      "          -870737600455895823],\n",
      "        [ 4288343883507671266,  6255578551198921952,  2716429306222093012,\n",
      "          2197385117988765178]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 3701425873623077417, -2785416995674118958],\n",
      "        [ 3131436260899300567, -8231716559199556861],\n",
      "        [ 4761263973767768107,   895688907941851881],\n",
      "        [-6049547285147554296,  1789843895485185368]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "Get attribute forward\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 6145106062772294150, -3780024118121061196,  7689609490617797863,\n",
      "         -5861391885217578225],\n",
      "        [-7295450244422314324,  5533276316057563312, -3327749906166023043,\n",
      "           870737600455904659],\n",
      "        [-4288343883507647784, -6255578551198879778, -2716429306222031707,\n",
      "         -2197385117988760277]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[-3701425873623079625,  2785416995674102076],\n",
      "        [-3131436260899325694,  8231716559199580052],\n",
      "        [-4761263973767755659,  -895688907941860843],\n",
      "        [ 6049547285147561323, -1789843895485185834]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 907619086950567037, 5503830038488078608],\n",
      "        [ 907645226435178310, 5503871852835359141],\n",
      "        [ 907619072210188432, 5503650938191568678]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")MPCTensor(\n",
      "\t_tensor=tensor([[ -907619086950562052, -5503830038488055030],\n",
      "        [ -907645226435156981, -5503871852835362065],\n",
      "        [ -907619072210174697, -5503650938191559135]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "\n",
      "Decrypted result:\n",
      " tensor([[ 0.0761,  0.3598],\n",
      "        [ 0.3255, -0.0446],\n",
      "        [ 0.2096,  0.1456]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def forward_single_encrypted_layer():\n",
    "    # Load and encrypt the layer\n",
    "    layer = crypten.load_from_party(layer_linear_file, src=ALICE)\n",
    "    layer_enc = crypten.nn.from_pytorch(layer, dummy_input=torch.empty((1,4)))\n",
    "    layer_enc.encrypt(src=ALICE)\n",
    "    \n",
    "    # Note that layer parameters are encrypted:\n",
    "    crypten.print(\"Weights:\\n\", layer_enc.weight.share)\n",
    "    crypten.print(\"Bias:\\n\", layer_enc.bias.share, \"\\n\")\n",
    "    \n",
    "    # Load and encrypt data\n",
    "    data_enc = crypten.load_from_party(toy_data_file, src=BOB)\n",
    "    \n",
    "    # Apply the encrypted layer (linear transformation):\n",
    "    result_enc = layer_enc.forward(data_enc)\n",
    "    \n",
    "    # Decrypt the result:\n",
    "    result = result_enc.get_plain_text()\n",
    "    \n",
    "    # Examine the result\n",
    "    crypten.print(\"Decrypted result:\\n\", result)\n",
    "        \n",
    "forward_single_encrypted_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the application of the encrypted linear layer on the encrypted data produces an encrypted result, which we can then decrypt to get the values in plaintext.\n",
    "\n",
    "Let's look at a second linear transformation, to give a flavor of how accuracy is preserved even when the data and the layer are encrypted. We'll look at a uniform scaling transformation, in which all tensor elements are multiplied by the same scalar factor. Again, we'll assume Alice has the layer and the rank 0 process, and Bob has the data and the rank 1 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a linear layer with random weights\n",
    "layer_scale = nn.Linear(3, 3)\n",
    "\n",
    "# Construct a uniform scaling matrix: we'll scale by factor 5\n",
    "factor = 5\n",
    "layer_scale._parameters['weight'] = torch.eye(3)*factor\n",
    "layer_scale._parameters['bias'] = torch.zeros_like(layer_scale._parameters['bias'])\n",
    "\n",
    "# Save the plaintext layer\n",
    "layer_scale_file = \"/tmp/tutorial5_layer_alice2.pth\"\n",
    "crypten.save(layer_scale, layer_scale_file)\n",
    "temp_files.append(layer_scale_file)\n",
    "\n",
    "# Construct some toy data\n",
    "features = 3\n",
    "examples = 2\n",
    "toy_data = torch.ones(examples, features)\n",
    "\n",
    "# Save the plaintext toy data\n",
    "toy_data_file = \"/tmp/tutorial5_data_bob2.pth\"\n",
    "crypten.save(toy_data, toy_data_file)\n",
    "temp_files.append(toy_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataaa encryptDataaa encrypt  MPCTensor(\n",
      "\t_tensor=tensor([[ 6455432689991242351, -8747987424833284864,   516194156793254464],\n",
      "        [-7184652485754408338, -5728763306424634317,  4126569007348696962]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")MPCTensor(\n",
      "\t_tensor=tensor([[-6455432689991176815,  8747987424833350400,  -516194156793188928],\n",
      "        [ 7184652485754473874,  5728763306424699853, -4126569007348631426]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "\n",
      "Weights:\n",
      "Get attribute  forwardtensor([[ 7684061974976171312,   682205155139273590,  4221196446478920753],\n",
      "        [-2590231570203555516, -5623409301982611713, -1273781711517600676],\n",
      "        [-1119078498499234934,  3370381831253906524,   -43614710911775527]])\n",
      "\n",
      "Bias:\n",
      "\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[-6455432689991176815,  8747987424833350400,  -516194156793188928],\n",
      "        [ 7184652485754473874,  5728763306424699853, -4126569007348631426]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ") \n",
      "tensor([ 7240571093388984395, -5701526885386818452,  3968381256011954826])\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[-7684061974975843632,  2590231570203555516,  1119078498499234934],\n",
      "        [ -682205155139273590,  5623409301982939393, -3370381831253906524],\n",
      "        [-4221196446478920753,  1273781711517600676,    43614710912103207]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")Get attribute\n",
      " forward\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 6455432689991242351, -8747987424833284864,   516194156793254464],\n",
      "        [-7184652485754408338, -5728763306424634317,  4126569007348696962]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 7684061974976171312, -2590231570203555516, -1119078498499234934],\n",
      "        [  682205155139273590, -5623409301982611713,  3370381831253906524],\n",
      "        [ 4221196446478920753, -1273781711517600676,   -43614710911775527]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[-7240671474700551300,  5701459898519365750, -3968325925307536157],\n",
      "        [-7240568255294341361,  5701598574007671266, -3968282417437096393]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")MPCTensor(\n",
      "\t_tensor=tensor([[ 7240671474700878980, -5701459898519038070,  3968325925307863837],\n",
      "        [ 7240568255294669041, -5701598574007343586,  3968282417437424073]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "\n",
      "Plaintext result:\n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def forward_scaling_layer():\n",
    "    rank = comm.get().get_rank()\n",
    "    \n",
    "    # Load and encrypt the layer\n",
    "    layer = crypten.load_from_party(layer_scale_file, src=ALICE)\n",
    "    layer_enc = crypten.nn.from_pytorch(layer, dummy_input=torch.empty((1,3)))\n",
    "    layer_enc.encrypt(src=ALICE)\n",
    "    \n",
    "    # Load and encrypt data\n",
    "    data_enc = crypten.load_from_party(toy_data_file, src=BOB)   \n",
    "\n",
    "    print(\"Dataaa encrypt\", data_enc)\n",
    "    # Note that layer parameters are (still) encrypted:\n",
    "    crypten.print(\"Weights:\\n\", layer_enc.weight.share)\n",
    "    crypten.print(\"Bias:\\n\\n\", layer_enc.bias.share)\n",
    "\n",
    "    # Apply the encrypted scaling transformation\n",
    "    result_enc = layer_enc.forward(data_enc)\n",
    "\n",
    "    # Decrypt the result:\n",
    "    result = result_enc.get_plain_text()\n",
    "    crypten.print(\"Plaintext result:\\n\", (result))\n",
    "        \n",
    "z = forward_scaling_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plaintext tensor is correctly scaled, even though we applied the encrypted transformation on the encrypted input! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Neural Networks\n",
    "Let's now look at how the encrypted input moves through an encrypted multi-layer neural network. \n",
    "\n",
    "For ease of explanation, we'll first step through a network with only two linear layers and ReLU activations. Again, we'll assume Alice has a network and Bob has some data, and they wish to run encrypted inference. \n",
    "\n",
    "To simulate this, we'll once again generate some toy data and train Alice's network on it. Then we'll encrypt Alice's network, Bob's data, and step through every layer in the network with the encrypted data. Through this, we'll see how the computations get applied although the network and the data are encrypted.\n",
    "\n",
    "### Setup\n",
    "As in Tutorial 3, we will first generate 1000 ground truth samples using 50 features and a randomly generated hyperplane to separate positive and negative examples. We will then modify the labels so that they are all non-negative. Finally, we will split the data so that the first 900 samples belong to Alice and the last 100 samples belong to Bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "features = 50\n",
    "examples = 1000\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Generate toy data and separating hyperplane\n",
    "data = torch.randn(examples, features)\n",
    "w_true = torch.randn(1, features)\n",
    "b_true = torch.randn(1)\n",
    "labels = w_true.matmul(data.t()).add(b_true).sign()\n",
    "\n",
    "# Change labels to non-negative values\n",
    "labels_nn = torch.where(labels==-1, torch.zeros(labels.size()), labels)\n",
    "labels_nn = labels_nn.squeeze().long()\n",
    "\n",
    "# Split data into Alice's and Bob's portions:\n",
    "data_alice, labels_alice = data[:900], labels_nn[:900]\n",
    "data_bob, labels_bob = data[900:], labels_nn[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Alice's network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Loss: 0.24704287946224213\n",
      "Epoch 199 Loss: 0.08965437859296799\n",
      "Epoch 299 Loss: 0.05166155472397804\n",
      "Epoch 399 Loss: 0.0351078175008297\n",
      "Epoch 499 Loss: 0.026072407141327858\n"
     ]
    }
   ],
   "source": [
    "# Train and save Alice's network\n",
    "model = AliceNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(500):  \n",
    "    #forward pass: compute prediction\n",
    "    output = model(data_alice)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = criterion(output, labels_alice)\n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch\", i, \"Loss:\", loss.item())\n",
    "    \n",
    "    #zero gradients for learnable parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #backward pass: compute gradient with respect to model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    #update model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "sample_trained_model_file = '/tmp/tutorial5_alice_model.pth'\n",
    "torch.save(model, sample_trained_model_file)\n",
    "temp_files.append(sample_trained_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping through a Multi-layer Network\n",
    "\n",
    "Let's now look at what happens when we load the network Alice's has trained and encrypt it. First, we'll look at how the network structure changes when we convert it from a PyTorch network to CrypTen network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 5 \tModule: Linear encrypted module\n",
      "Name: 6 \tModule: ReLU encrypted module\n",
      "Name: output \tModule: Linear encrypted module\n"
     ]
    }
   ],
   "source": [
    "# Load the trained network to Alice\n",
    "model_plaintext = crypten.load(sample_trained_model_file, model_class=AliceNet, src=ALICE)\n",
    "\n",
    "# Convert the trained network to CrypTen network \n",
    "private_model = crypten.nn.from_pytorch(model_plaintext, dummy_input=torch.empty((1, 50)))\n",
    "# Encrypt the network\n",
    "private_model.encrypt(src=ALICE)\n",
    "\n",
    "# Examine the structure of the encrypted CrypTen network\n",
    "for name, curr_module in private_model._modules.items():\n",
    "    print(\"Name:\", name, \"\\tModule:\", curr_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the encrypted network has 3 modules, named '5', '6' and 'output', denoting the first Linear layer, the ReLU activation, and the second Linear layer respectively. These modules are encrypted just as the layers in the previous section were. \n",
    "\n",
    "Now let's encrypt Bob's data, and step it through each encrypted module. For readability, we will use only 3 examples from Bob's data to illustrate the inference. Note how Bob's data remains encrypted after each individual layer's computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Select only the first three examples in Bob's data for readability\n",
    "data = data_bob[:3]\n",
    "sample_data_bob_file = '/tmp/tutorial5_data_bob3.pth'\n",
    "torch.save(data, sample_data_bob_file)\n",
    "temp_files.append(sample_data_bob_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get attributeGet attribute  forwardforward\n",
      "\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 1930858796196566520, -3144156726349377349, -8307232669664963043,\n",
      "         -1133451210449637043,  5654385132561726571, -1043789476888633108,\n",
      "         -6995525753409473009, -3997261881799917472,    13455572892314038,\n",
      "         -7639031434956488127, -2393301928545047453,  3074307310477075710,\n",
      "          3726817759688948116,  4181208729066717746, -7913699261352063241,\n",
      "          5789769628045013939,  5793823176962135247, -5517723517497269594,\n",
      "         -3078879589518540643,  2483854674813001856, -3349596167974936281,\n",
      "         -3229986890523988216, -8105764765296963736,  5542982851805905947,\n",
      "          1582617541676294862,  3183585594303886352, -6709576745326183893,\n",
      "         -5368986162118089957, -8990442957359876259, -1333889376328300424,\n",
      "         -1219061707078173498,   724348871163094774,  6698247105132111599,\n",
      "         -5917535974287179257, -4641570051954164620,  1180569505411148763,\n",
      "          4099135254593847336,    68474902992562351,  9177425831406887686,\n",
      "          8043041169519151046, -1209509009163111707, -4055931747537415791,\n",
      "          6381630321656595544, -2272290476691111247,   688174768056749637,\n",
      "          -666234652952746026,  1608799115741978218,  6959038707782960150,\n",
      "         -6700062480545970771,  5990762751767143414],\n",
      "        [ -911104274238019295,  3854534443006008341,  7648600150370509082,\n",
      "          1470749788857653401, -4471909048957186058,  7916665985816494941,\n",
      "           323398431964915048,  4446461975691933612, -8820107742341108922,\n",
      "          3986469104167306444, -6688378608481440991, -3507882209098191744,\n",
      "         -8863103531575306430,  -188841853036423887,  -239094728483351789,\n",
      "          2582876945424764338,  6276663426565863961,  -494327240334645656,\n",
      "         -3355447861625608526,  2474401344889691792, -7987170336839208410,\n",
      "          3836219949206955925,  2720984089283912649,  1826205544376469278,\n",
      "          1217467282030244024, -9188621077895995302, -6085324199000903272,\n",
      "         -1864203766284549420, -3512527120008068923,  6549536315565646692,\n",
      "         -1909667645288508347,  8429546885666453096,  1491176395284272816,\n",
      "         -4663327438248981975, -6160497388716557121,   510668826992452235,\n",
      "         -1948087093395907045,  -413419685095963505,  6393119249822957983,\n",
      "          6065150017801374421,  -884261602790214492,   816641121873728744,\n",
      "          8854073499177154587,  1800843614655933946, -9116454747852591209,\n",
      "          6355541322384017443,  2600126115093517267,  7437819770110914837,\n",
      "          7035303791793807337,  1406741365254374238],\n",
      "        [-5069258257487834312, -1044499013086119957,  1331940458843160166,\n",
      "          1069923710100111070,  1286079433999531704,   -93858478875835006,\n",
      "         -7982594687812348525,  -478577077494604308, -6404284840523511309,\n",
      "          1355055733054827036,  -408865538553245412,  6500912212472389564,\n",
      "          3384258155872132288, -4868032433927310883, -1621341285645448969,\n",
      "         -2103421877032593805, -9180671391913252767,  6373685701650098682,\n",
      "          8008558936041085713, -8859003994685753118,  1349135089708947617,\n",
      "          3266000760695810074, -7514664437617048766,  -647940467909739878,\n",
      "         -1642994766989269527, -9023675818875988637, -1134215054557750671,\n",
      "          3729907330808875673, -5702832360435736669, -5733506645247562632,\n",
      "          1579404694416745476,  5702877753345758008,  6348442244754483717,\n",
      "         -6771127232823236929,  5703955067382303376,  4675365905300948160,\n",
      "          2096250412568833528,  8795206639260073263,  8155341460774360164,\n",
      "         -6029257563725453823,   866490979810428206, -5356380138222525410,\n",
      "         -4505482860174704904, -6863555310659482198, -2460578508321371049,\n",
      "          7702379224227661970, -7778447039127712485,  7186543419958319883,\n",
      "         -9180112769198551014,  5648285812442599627]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[-1930858796196559311,  3144156726349332761,  8307232669665100549,\n",
      "          1133451210449598363, -5654385132561816670,  1043789476888534753,\n",
      "          6995525753409449914,  3997261881799982847,   -13455572892369026,\n",
      "          7639031434956508739,  2393301928545033084, -3074307310477136333,\n",
      "         -3726817759688932451, -4181208729066833832,  7913699261352085793,\n",
      "         -5789769628045061588, -5793823176962091959,  5517723517497312399,\n",
      "          3078879589518469375, -2483854674813070869,  3349596167974934900,\n",
      "          3229986890524007228,  8105764765296958553, -5542982851805989263,\n",
      "         -1582617541676333054, -3183585594303941079,  6709576745326150091,\n",
      "          5368986162118006575,  8990442957359902604,  1333889376328384400,\n",
      "          1219061707078105546,  -724348871163013242, -6698247105132206242,\n",
      "          5917535974287196340,  4641570051954248238, -1180569505411142455,\n",
      "         -4099135254593858560,   -68474902992597643, -9177425831406981103,\n",
      "         -8043041169519184312,  1209509009163088188,  4055931747537416402,\n",
      "         -6381630321656615698,  2272290476691183577,  -688174768056713427,\n",
      "           666234652952840274, -1608799115742029711, -6959038707783054463,\n",
      "          6700062480545985536, -5990762751767144661],\n",
      "        [  911104274238045812, -3854534443005967587, -7648600150370452612,\n",
      "         -1470749788857610147,  4471909048957089878, -7916665985816518481,\n",
      "          -323398431964924558, -4446461975691921703,  8820107742341217683,\n",
      "         -3986469104167338055,  6688378608481468757,  3507882209098160936,\n",
      "          8863103531575252436,   188841853036399925,   239094728483366381,\n",
      "         -2582876945424784343, -6276663426565846803,   494327240334771033,\n",
      "          3355447861625577448, -2474401344889590621,  7987170336839218811,\n",
      "         -3836219949207010138, -2720984089283927905, -1826205544376294262,\n",
      "         -1217467282030232875,  9188621077896020846,  6085324199000990662,\n",
      "          1864203766284640570,  3512527120008010343, -6549536315565623076,\n",
      "          1909667645288545189, -8429546885666470646, -1491176395284294520,\n",
      "          4663327438248951338,  6160497388716539795,  -510668826992426746,\n",
      "          1948087093396088779,   413419685095950304, -6393119249823018045,\n",
      "         -6065150017801298902,   884261602790175276,  -816641121873868985,\n",
      "         -8854073499177033434, -1800843614655904350,  9116454747852552252,\n",
      "         -6355541322384071586, -2600126115093405043, -7437819770110822493,\n",
      "         -7035303791793801947, -1406741365254495551],\n",
      "        [ 5069258257487796684,  1044499013086192739, -1331940458843251450,\n",
      "         -1069923710100285943, -1286079433999580403,    93858478875896023,\n",
      "          7982594687812301974,   478577077494605557,  6404284840523374976,\n",
      "         -1355055733054616614,   408865538553217675, -6500912212472395485,\n",
      "         -3384258155872203838,  4868032433927377736,  1621341285645419132,\n",
      "          2103421877032620348,  9180671391913283186, -6373685701650095056,\n",
      "         -8008558936041048318,  8859003994685830970, -1349135089708977914,\n",
      "         -3266000760695785393,  7514664437617047814,   647940467909846378,\n",
      "          1642994766989310869,  9023675818876044935,  1134215054557838669,\n",
      "         -3729907330808904626,  5702832360435804857,  5733506645247512747,\n",
      "         -1579404694416691554, -5702877753345790522, -6348442244754442644,\n",
      "          6771127232823202531, -5703955067382284432, -4675365905300939781,\n",
      "         -2096250412568822672, -8795206639259956747, -8155341460774455895,\n",
      "          6029257563725375897,  -866490979810446896,  5356380138222584735,\n",
      "          4505482860174743446,  6863555310659501769,  2460578508321387670,\n",
      "         -7702379224227579017,  7778447039127732572, -7186543419958295550,\n",
      "          9180112769198607536, -5648285812442546410]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 2138267839993134774, -6232525761687493618,    37186052760207719,\n",
      "         -7643898555223852718, -2058636245073361556,  4959149682788799074,\n",
      "         -3980999018101732815, -4569166311984255651, -6099735171050142063,\n",
      "          3428035850636070133,  2825526036691422122,  8936623199995887705,\n",
      "          3508914960827409055, -4628985248928171722, -2641293644979171272,\n",
      "         -1909703528568066653, -7126066648434549423, -2605128644628281668,\n",
      "          4877750694853984907, -1979342529003558491],\n",
      "        [  -51033500408071167,  7579748500528674362,  6207366836920513085,\n",
      "         -7034976646051165352,  -103246033564107076, -5549788612483333743,\n",
      "         -6756044826896844238, -2595913215783267784, -2988437375558031282,\n",
      "         -5450368329466228348, -7300337537563740673, -7568603248944539894,\n",
      "         -1041261776262267608, -5997220946845856857, -5184613179297635083,\n",
      "          7681452786202192522,  4770179032539344974,  5988426183702694032,\n",
      "          4046347859744047044, -1402648264601909232],\n",
      "        [ 3657001648114922891, -6519930155114044569, -5325805802632266555,\n",
      "         -8749915503910993396, -2491669274684501821, -1449503563531034645,\n",
      "          2832002878905209297, -4730600908501114664, -4588024049142848338,\n",
      "         -1703178227464998952,  7579153636718438505,  1312680218525860678,\n",
      "          2823117755046548442,  5532386766558797731,  8826570648578355355,\n",
      "          4123308294241742762, -3163781996568009048,  7094667924966294590,\n",
      "         -7920286712389874980,  -263169409391270164],\n",
      "        [ 6247647629585512633,  4979698983383351033,   101309540769824166,\n",
      "          3298807883287990847,   371304148899081116, -3631567104612812983,\n",
      "         -6600012114804264661,  9095379901328065509, -7012914693473944922,\n",
      "          -201026823771748227, -7592643349365258202,  6531662885730516362,\n",
      "          7983456341774153289,  7115427990884953789, -8936712919372072598,\n",
      "            90805245076181425, -8781043943490570494,  8218545261846480425,\n",
      "         -7089846159916125558, -4213954302970720491],\n",
      "        [ 7171004736542195830, -8473697520249548238, -9187835228143417986,\n",
      "          -400430172941815500,  7887770033039693518,  1915656817848796973,\n",
      "          -146908310488253051, -7494520906810387253,  1502406572351571483,\n",
      "         -2886594096551045842,  8709915180506533325,  6873328181645258201,\n",
      "         -6687187296247565389,  3607484967720681191, -2701272704715729341,\n",
      "          4415941236208221055,  -665050454100774862,  6741161501742973398,\n",
      "          5587269491015978685,  4581405193784982393],\n",
      "        [-7965934322742802961,  4220916451636500133, -2543853615709496642,\n",
      "         -3989114663378793077,   232496029374516747, -1135688758248314507,\n",
      "          1329084094292609894,   280967945312880894, -2861159623191962912,\n",
      "         -5691348132888913206, -3775684390849412073, -3193156693648420349,\n",
      "          6890448160150419321,  4502163779660207135,  6219081194101314583,\n",
      "         -2915532430416602480,  4871352303922998354, -1583575088075038073,\n",
      "          6369925783273802835, -2116084725328259598],\n",
      "        [-8646075179073353242,  6012115500862792542,  7695037160200148928,\n",
      "          8169556130727282708,  7666593528571452311, -9159409240176053605,\n",
      "         -7688955190549250949,  1166807882868010986, -1778844680964443094,\n",
      "          -661617157574336853,  6742732957288046423,  2663604104235598678,\n",
      "         -2845220725608117811,  4463550834140101798,   -87041874796105952,\n",
      "          4168744363950791785,  5569888896576929183,   -85246737548933641,\n",
      "         -5117266237363681145,  8523969051953664685],\n",
      "        [-1417686396845374409, -8578983045271219529,   325427707970534834,\n",
      "         -5230694813940511194,  6619976222734097902,   574379264012763290,\n",
      "         -6725159631059267310,  2764465048554682602, -4202333460695332563,\n",
      "          6268579866839731594,  3536796191263638476, -2021297733215085745,\n",
      "          8507233252264560881, -2187773092513909616,  2176817745722591529,\n",
      "          7706068469076502357,  4263823008837243785,  4986410135498869762,\n",
      "         -6056101760276366120,  5436003930669773851],\n",
      "        [ 3991849880646406560,  7441600450627221344,  4839827653917352674,\n",
      "          5943166383402751830,  7882786240734659234,  1252121854881631593,\n",
      "         -5016722064556081547, -3238962863953834918,  1787149007161681856,\n",
      "          7355912605218530778, -3679398210220728681,  6139445604878360540,\n",
      "         -6924723336870744535, -1971119922088029368, -8269014641638146153,\n",
      "          7972167298419503436,  3324927999971200452,  7308135251512425211,\n",
      "         -1070791780668900002,  5641814352431749290],\n",
      "        [ 4572200648883327488,  3600488752290903328,  1769592825131560288,\n",
      "          4386580693657234477, -1749755666586977159, -6086011952390354483,\n",
      "         -5925174907496735175,  4991174637870550146, -8732162387944224866,\n",
      "         -2859002031052155122,  5598014108070863402, -6427556110485143348,\n",
      "          4208913904164565556, -7806893888526687286,     2285523721687470,\n",
      "         -8633449132370729760, -7227000367975623466, -6004824044585422885,\n",
      "         -3243210270428149342,  -619193148636571912],\n",
      "        [-2875152542028296354,  8885451808329026879,  -924994353828860310,\n",
      "         -7245285682822411637, -1418512364566264799, -7374871067593983662,\n",
      "          8131774864716535887,  3217175478052880374, -4629565759654741008,\n",
      "         -8747260614518680726,  -689168540893079884, -8560291934283934095,\n",
      "          2454514305747686289, -3811493400970099625, -7686644097165400186,\n",
      "           263585188711320033,  1890040889308802901,  6630606114156854717,\n",
      "         -6413544535472716049,  8646862552205643885],\n",
      "        [  996963643372430795, -7703299726345236358, -5714539980537827029,\n",
      "         -2220714484199674085, -2926259514403701961, -1349736466556984708,\n",
      "          7583123771321192538,  5080718685574439248, -4504842576805644181,\n",
      "          1507162235352185940,  2186229308719834284,  7843536632408220992,\n",
      "          9103755107675811880,  7493180691204012503, -4975089939683563649,\n",
      "         -1292749358039437194, -8724759943018259874, -4488405512256946639,\n",
      "          1811143244435946581,   405932914037024124],\n",
      "        [ 8373202287487884150, -9064508624018631663,  7578708979169686167,\n",
      "         -7057079406224715605,   -73435476824712394,  5613720612687199511,\n",
      "          5430931466241375048,  6358354282252918269, -2865649734728872826,\n",
      "         -1773889645625595057,  8959756410880981056,  6439042711092596484,\n",
      "         -2966339476274140967,  7454314673830781743, -1329971158818482188,\n",
      "         -7423415730302851815,   179166948871758150,  5784954529024077189,\n",
      "          3737935496488437474,   499974894190197927],\n",
      "        [-4082756575534187315, -3737601999682009793,  3159240727232324045,\n",
      "          7760617415440815334, -2114640853025740913, -7386977693739569623,\n",
      "          8249114747658950301, -7140073809050233450,  9062846008873286088,\n",
      "          8634457926088321843, -6564996739060978554,  5161695497476086947,\n",
      "          8166314995771025257,  7582513177752947006,  8264599060712665525,\n",
      "          5964405964427844813, -2039930743892095310,  6158089241998382769,\n",
      "          1645547449704538000, -3761992200738143390],\n",
      "        [-2790422309770678274,  8311206840323600115,  6848380402247477674,\n",
      "          1006812251806883529,  3999089914536451891,   779673426918463450,\n",
      "          -104931887338781544,  1236605616561815705,  8716185562402131767,\n",
      "          1533103001517932488, -4428360429247494671, -2702621152843329211,\n",
      "         -7401064865401674019, -4245654705027382075, -3153519617659351106,\n",
      "         -1390092976192868922, -8944798051231622033, -1577253449072714009,\n",
      "          7712420828355431921,  8832832220547737332],\n",
      "        [ 8405742858797974162,  6801607493843847985,  3910360650174225055,\n",
      "           505990926354836527, -4751216930170479273,  2022344978451005083,\n",
      "          1761211961179178570,  7117575952431216251,  -281778279973010348,\n",
      "          9028470159097920103,  1680969166679417533,   396824213938174330,\n",
      "          6444917087274622649, -8046880103271052315,  9200487948586369691,\n",
      "         -8283974410804576112, -5732789994409093472, -1033521349951954550,\n",
      "         -8016634316321619070,  2868623473959612494],\n",
      "        [ -957427991646971065, -2004836335198305672,  -619748829487049778,\n",
      "         -2941856298719973297, -7207820826800356211, -7784286632398761463,\n",
      "          6874528983167909376,  5832626949372946206,  1877865270204827894,\n",
      "         -7586477399159242489,  3148957174196142438,  7262834608156432021,\n",
      "          7769730457025608945, -3168430209441436004, -3733228283891989797,\n",
      "          9058527231699274531, -2079775055848035475,   435025766630359000,\n",
      "          2877568556224446711, -3022446473639680965],\n",
      "        [ 6943007529557349934,  7100220140610472551,  5622993673948621846,\n",
      "         -3780290539885298635,  5605291492765781281, -6257183375504872993,\n",
      "          -409892757626747623, -4778285622030953954,  1853943783793992083,\n",
      "           383277353162276550, -4019532551791638052,  8785988959942797886,\n",
      "         -5268307178679778478, -9209090163789814371, -9060673827598787684,\n",
      "          6015832432069771447,   -53743464618727184,  4365965156490554494,\n",
      "           605923772142806776,  6928237112926509082],\n",
      "        [-8840179004310987508,  1995967236976290304,   278214651373346821,\n",
      "          -643093894623595156,  2546994360070112511, -6879189766231375238,\n",
      "          1802937763163112938,  5509580998098256821,  2937406582478630700,\n",
      "          8199432291664741544,  8091093876261147911, -3277859874744147341,\n",
      "          2185103913797659811, -5547778110676781110, -6267171332406765542,\n",
      "         -1004878307630174736,  1581935767638021353, -8443477149161463155,\n",
      "          8444560365008125151,  8645383103941550154],\n",
      "        [ 6122532156654196216,  9095283532684966282,  5785846390907959134,\n",
      "          7955569284720745321,  5988849580321760362,  6833062242729837778,\n",
      "         -2706319606811707437,  7875315535986713735,  1683563042405843449,\n",
      "          5151397253543596870,  4457984430673425182,  2463439611011369906,\n",
      "          -756356335308206733, -1566129713918514726,  2457141219505228582,\n",
      "          8613420810267066925, -1180970954077039433, -6789648979035677766,\n",
      "         -6984296901300687284,  1656564697650077216],\n",
      "        [ 4064404292816705131,   835039007293712611,  7601106679700714706,\n",
      "          3516459821535214153,  9066518829672431213,  7730510890705985313,\n",
      "         -5752473548161896490,  -202036300045584613, -7693707815595574735,\n",
      "          7437365103661763808, -6373177503111646977, -3092009215960549982,\n",
      "          8366515114254200990,  -508009445713909744,  2472486094824512008,\n",
      "          3560809682654245438, -3035100214719347954, -3629505742322953264,\n",
      "          8673315723238911603, -8742611339301547816],\n",
      "        [-7134850171058820754,  8157568061480720771,  4223758466847094081,\n",
      "         -5604006296242104555, -6332375037283414027,  -696656738104454121,\n",
      "          4824615466429397230,  7206521930613852142,   878078628028622510,\n",
      "          7575182304694522924, -8232497110958954227,   424238684042394323,\n",
      "          2784371982104837113,  8952780356503979525,  2702994771668032314,\n",
      "         -5015094869198503343,  -786333992610483878, -3268688809067017375,\n",
      "          9052283200101757078,  2122508808419617437],\n",
      "        [-5381002706292771838,  -707586275247383861,  8339842300027926909,\n",
      "         -1234232394876927708, -5225148030892219052, -8515429204497861582,\n",
      "          8436726506705426041,  8165719451269906348,  1463603724583602092,\n",
      "           198079988243617679,    19422278717768629, -3822080868959645706,\n",
      "          1319542375289648479, -4010864646143278572,  8983131970626304414,\n",
      "         -6947547644751656555,  8660214074772075145,  7169575259495530163,\n",
      "          7175507392209289957, -7018772429809177512],\n",
      "        [-8884833050837222465, -1949070386235451963,  2955796546859993575,\n",
      "          2265814675548905955,  -702287290317083253,  7644550662320014853,\n",
      "          5218737643842802584,   348033307790443042, -2023008758327573355,\n",
      "         -5538140565880209080,  1095682852601545560, -2531421413834352727,\n",
      "         -6151671824398630685,  1404105683758782292, -3118311870615163707,\n",
      "          3743152077674723973, -1078392297881130579, -8645347360667626300,\n",
      "         -5392699730492763561,  5437516266579984993],\n",
      "        [ -376326605633896716, -8545374120986752946,  3154429211176615570,\n",
      "          5802715216584264015,  -477799122001871058,  1770771300647757949,\n",
      "          5507804623285439180,  7820228786090122446,   903766486585864257,\n",
      "          8797632728394713580, -4183929870172131546,  5523438981960522168,\n",
      "          7601130865353183811, -7316522329496602692, -1824475303906620776,\n",
      "         -3794124600097064328,   965801424459088946,  4430307417474185931,\n",
      "          5742605053296515923,  5808368886514977802],\n",
      "        [ 2394206717698160147,  9079154816450903039,  7686097062453668984,\n",
      "         -2803620475173843034,  7121048462722536246, -4246058021573666369,\n",
      "         -9142238981435453818, -9063605641734689113,  1195323790178013441,\n",
      "         -5649232026764136475, -5991722870137600127,  4975286066847516463,\n",
      "          9206632441638861397, -4718250908301088726,  8676662669041563671,\n",
      "          4566311693274295588, -2155740476284636248,  1354348339262109951,\n",
      "         -4518674962771434931,   319536546626369002],\n",
      "        [-5475047707243058909,  7532707314008349820, -6277922398964432769,\n",
      "         -6431749217830787981, -7308579197276445831,  1712737664475518974,\n",
      "         -8982970104256480672,  6927951289440962043, -5087434783157522599,\n",
      "         -1257299883632268313,  3911937387247569170, -1603668561510677063,\n",
      "          7295424588016645688,  3194922762929952033, -4582925991684930066,\n",
      "          6014065748853288722, -6984402313457911197,  3274113577101786532,\n",
      "         -7231690846837524642,   592959094690985429],\n",
      "        [-2548189519701268780, -2779504299078731940, -3274358405103504142,\n",
      "         -5478316205378405741, -1214511161750087992,  8262904761898526252,\n",
      "          -497386680824199951,  8603135498224542538,  3193884981863179118,\n",
      "          1545068316016938810, -4675049791052320766, -7928525683059540197,\n",
      "         -9116596621534774315,  1751608545374341942,   323603492560580379,\n",
      "          1760830661199882121, -8271075323231719435,  6773413714471398312,\n",
      "         -8756061253084282675, -6896938723680741500],\n",
      "        [-6149737682433943196,  6907439395165712208,  3887033591046505662,\n",
      "          2066810258113012493, -9143452366531357733,  8055236285859938263,\n",
      "          6979510845582730425,  7273736346297538801,  2992565522553243870,\n",
      "          7948945688597906349, -6375883769782073132,  5049318446170840638,\n",
      "          2063699186974914022, -3940488550207298558,  5810805510191778816,\n",
      "         -4646131094028956372,  7840417913175873244,  1312946525446269508,\n",
      "          4316791857949311816,  2956028254705758217],\n",
      "        [-9134892326225598646,  2824978925210321225, -1505683568170662142,\n",
      "          8465332735544673529, -6689392041977835265,  7175384500592549469,\n",
      "          2391940056427727107, -3763711980929478734, -3858412703590687745,\n",
      "         -2028726660551093808,  8303394203710544830,  5225000377583637459,\n",
      "          1110344666656559065, -4571110083930509103,   195810311890309056,\n",
      "          9125546541035014258,  8284695523422890767,  1467465416014007480,\n",
      "         -4339485663215230358,  5052186224514195937],\n",
      "        [ 3385440924056748117,  1327738924829088543,  2501980052449215966,\n",
      "          7127633348039717158,  5868916595381511272, -4174723263015726106,\n",
      "          7422631061666911051, -5382459294302600807,  7010966739554722269,\n",
      "          7453121925895154313,  1255954828680318056, -7706131853270753327,\n",
      "          5658166379153945762, -1997115832696525691, -6414914588430219162,\n",
      "          5685795017961430790,  7568407182947557317,  4766975264793062744,\n",
      "         -4783213200056828146, -5932012444352090910],\n",
      "        [ 3242051076432590273, -5595944546383841498, -7565985936179039424,\n",
      "         -1668089270941420047, -6131603690155078980, -1853428563004833953,\n",
      "         -7551131888688807022, -5471646289223675545,  1316551968675717212,\n",
      "          3069619378839833246, -3293495506111507594, -6073706232922914946,\n",
      "          3395340910594543645,  4581174716382175628,  9185905767816474555,\n",
      "          1732640561697549783,  -417448532745995350, -7996009747162738187,\n",
      "          8972789571254191070,  5826688315645657687],\n",
      "        [ 6834182050327498959,  6091922599652497597,  2937459750769923381,\n",
      "         -1370772276913576116,  6291202772558999621,  5213087303688832188,\n",
      "         -9085294118474965011,  6125231537425443635, -4843658785707516770,\n",
      "         -6765554540341394506, -2192563368037486876, -7185719158167511638,\n",
      "         -2231502190536828932, -8061309003049242333,  2204776993905297195,\n",
      "          6673170425490987476, -1370053508556210814,  1230237934581567346,\n",
      "          8737176663885251244, -1497784470140822128],\n",
      "        [ 1192384616141061232,   195433013324279435, -1259178853300948771,\n",
      "          7828948346105094434, -6793278115758039508, -6317041225311931414,\n",
      "         -2787751892118748322,  1111333503583370084, -7127542217523154261,\n",
      "          3853227055193736351, -4436266946156352881, -7583358428569145530,\n",
      "          3532150444129500207, -6976365497880773884,  7463008958984160328,\n",
      "         -5827742420590768328, -2142037726066174726,  5476377932498476734,\n",
      "          3336220864220763365,  2888458212027213404],\n",
      "        [-2259733146140540190,  4196501058340302822, -4074224620323252966,\n",
      "          -980274962269399927, -3670751964142459723,  9211405537583011401,\n",
      "          1346055695108571350,  -109129218784018567, -4486719120822271394,\n",
      "         -7774343746051367867,  1454146247440539876,  4404318275214469508,\n",
      "          -383292964189593775, -7030937772563163541,  5925335668100022828,\n",
      "          4233598085829534951,  8086511002291719335, -6124468658515182864,\n",
      "         -1986476220081849807,  -191566803917543232],\n",
      "        [  -43375543615693359,  4200690427526066219,  8386075656674463773,\n",
      "          -420830846036166379,  5906414232757545099, -7619034089374785218,\n",
      "          1210016022621572306, -3644056818182502113,  -190325748217556624,\n",
      "          -804912791247029054, -1337071808029911944, -8747535359291562785,\n",
      "         -8589767585518063099, -7346920963626418159, -5443287822950756780,\n",
      "           904637167619601476,  1945263472381307841,  7872801629213163796,\n",
      "         -8326662702027155568, -2779431963200879322],\n",
      "        [ 7652961635074032139, -4482382067448658476, -7185759163404829645,\n",
      "         -7227806497273653015,  2792274773847383604,  1976589780351060796,\n",
      "          7823352225593561987,  2466225235101961464,  5542003671167592849,\n",
      "          7907130384111424434, -6817309670255744968,  2185224169227894369,\n",
      "         -1774492960428892202,  5017977051184389483, -3776572588087354694,\n",
      "         -2182808959593837557,  -643126301715964432,  1237763226861110622,\n",
      "         -8213193799290962565, -6146696629987808444],\n",
      "        [-2503964471394297631, -5366743791212765637,  6895354419141893465,\n",
      "         -8460985143556392048, -7105406182887107501,  3345204583401633228,\n",
      "          1959326056148287502,   235499204522378768, -5060316032347408931,\n",
      "         -4053445742262481190,  7630679360665421737,  8812582305428987809,\n",
      "          9033890982854008578,  -311697853617479324,  7021126400888842367,\n",
      "          1497037821395800687, -4537480507211161060, -1472668446989060415,\n",
      "          6221329463149327396,  6700941123548589044],\n",
      "        [  383843921234957970, -1392754746213463455,  4950921809411658792,\n",
      "          5521542365231684739,  7439297545151752181, -4386759413396179600,\n",
      "          -388257143537698575, -1176920536610994497,   633612740462923182,\n",
      "         -4688753753057287050,  1603019259992896197, -5165931586336879315,\n",
      "         -1601613240631923100, -2679344237019510027, -5029402664230854314,\n",
      "          3203819764214645000, -4970187255762750195,  6543041436630329526,\n",
      "         -6877207075301863572, -4914494680125968521],\n",
      "        [ 4769330917262723284, -1765497962469946748, -1799322509984105264,\n",
      "          1108335122236451825,   299766377060088331,  4641572140556328032,\n",
      "         -3928647915232544595, -5173287117484615421, -7797650945264403137,\n",
      "          5590200814467688212, -6986557303945843639, -6352647469831309308,\n",
      "           799016832463457977,   401393649303164617, -4485817280349723738,\n",
      "          8469596769588797322,  1747919027053054618, -7241088400568182630,\n",
      "         -5321023474564130538,  5738666012159183569],\n",
      "        [ 7908276788999196397,  7328308325909273847,  -190534744369137843,\n",
      "         -2120030297683958606,  5730827441996852593,  1977773848953749734,\n",
      "         -9141789702553699305, -2849260551220173027, -7265251627578011444,\n",
      "          1886897381886709149,  4591512681834861775,  6412032495129624658,\n",
      "          4043929904671859175, -7537257234267382602, -4032220343739452567,\n",
      "         -4494424059908873306,  3836350598664293709,  7037366800826853003,\n",
      "          8498186590734529578, -5910640029348867418],\n",
      "        [ 1898899788146374840,  2281415385845834064, -6192680664898537244,\n",
      "          2848875191463171508,  3848260571886568190, -4710081289300606567,\n",
      "          1009539374625085532, -7869473932122629247,   230203422097546213,\n",
      "          1622301583734062033, -8596743280346794711,  2391582736141364326,\n",
      "          1093028290388520068, -6427721011092631755,  1154654285088645896,\n",
      "         -5183061710791681440,  5487999871965551237,   393871104732095205,\n",
      "         -4678906034073110659, -8643645450827421701],\n",
      "        [ 6175940445762130513, -1632709014389986456, -8895485468703863081,\n",
      "           169578649487415755,  3883427907408516460,  5654348740159226904,\n",
      "          -502430761244862804,  3190135804215422128,  -180242306794805908,\n",
      "         -4506995124208731738, -2557952700053035250, -7046247172164729649,\n",
      "          -189949406237557722, -1548268109707028840, -3981457525660411021,\n",
      "         -1611711005157659301,  2286872762921430880,  -200605057760009923,\n",
      "         -2398082627190570859, -6639136739911868998],\n",
      "        [ 7051247254792411986,  8150324123449881037, -8742458676233556044,\n",
      "          6193088797000167044,  2757619578482676832,  3782957945793507787,\n",
      "         -8086598196172329047,  -479642860904144132,  1105748029894719662,\n",
      "         -6692462672342219854,  2902463690111166105, -2486726790760752069,\n",
      "          5573765846390146586,  5366387652425213900,  7499960882292405989,\n",
      "          6832890653238877593,  2173667483515890736,  2173518358388259510,\n",
      "          1866406869128682610,  4109086349522562059],\n",
      "        [ 8052802894859306322, -8680892794167843381, -8949885447346365280,\n",
      "          3321606721399223784, -9126089386389549218,  -494819621869965895,\n",
      "         -3634987423224846635, -3542235320615663385,  5880546709688962938,\n",
      "         -5719753414260741230,  7334033505267295933, -8171847960891395295,\n",
      "          6620153564240175623,  9106826080714642499, -7760022458795284914,\n",
      "          8317421324084863383, -4897794570072986918, -3444131106716846628,\n",
      "          6719107595420283618, -8060867267452073401],\n",
      "        [-8341150333456781952,  -485667059363920508, -6711196348661175499,\n",
      "          5330331620885796847,  6431439230353320057, -5821688665117191594,\n",
      "          4117028686283191215, -7844721201449336106, -2912787967240487059,\n",
      "          -298851223056251625, -6170173265738909443,  2086041381460809135,\n",
      "         -4628066104944391872,  9160695375060863939, -8730098563056874905,\n",
      "         -7318130137456938797, -3478439040528752742,  6107284621750999834,\n",
      "         -3854981367652503331, -4432057281187691749],\n",
      "        [ 5769459013034087306, -8780019039463518086, -3205511263429189900,\n",
      "         -6660070787558164102, -5582000366295531017,  2458044319871737934,\n",
      "          -931605516809441576,  1926240395414148453,  3394559099647900796,\n",
      "          6695010663333831049, -4945726213062068899, -8354427615920675139,\n",
      "          5400358943946827757,  4941774936346618331,  4750055334973371668,\n",
      "         -8012287759178338871, -1243620145552456995,   778341804692599722,\n",
      "          4708302134723316693, -1585649946527618343],\n",
      "        [ 5952475353189377256,  1325472467235384623,  6306211660091572515,\n",
      "          6276316190335627852,  1087867177613143883, -1887450314217256770,\n",
      "          -589585565603097802,  2156767586574137764,  6028560678323797828,\n",
      "          6776360141017583125, -4824751037029416611,  2461904886614949844,\n",
      "         -2691075456992490060,  -269269076707837443,  3411941775764840696,\n",
      "         -5641973149079099643, -5093593549792568429,   732066163530569645,\n",
      "         -9011288301568037049,  4005448716457627814],\n",
      "        [-5875821436152523478,  4540539976653510361,  6711194299840455493,\n",
      "          4340249179838930602, -2970866482920951968,   562186443403806420,\n",
      "         -4591823827000539544, -9038991139374716406,  3096108488627372795,\n",
      "         -2049471177413908448,  3411333509251273820, -7588303354016816300,\n",
      "          2156487147257909951,  -943967810689417463,  3634617919587867015,\n",
      "          4499795848986627018,  -211450814595091198,  6176802610071215200,\n",
      "         -2891174351740791333, -7655379638625215824],\n",
      "        [ 2872157386517029722,  9005578491756540607, -4529939714124829513,\n",
      "         -2273998201736683789,  8615246842344408249,  6632230416440556759,\n",
      "         -7818173164028855381, -8569730384212181042, -6668510849803535263,\n",
      "          1371037162608498040,  3422938739454622236,  1565440657774801322,\n",
      "           444126252663832050,  1628369401833138782,  2508725965669938977,\n",
      "         -2201894812335445725, -3419748837528257734, -1092351858843830367,\n",
      "         -3124776907674591572,  7763776655640001404]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[-2138267839993145653,  6232525761687497584,   -37186052760204217,\n",
      "          7643898555223847616,  2058636245073369109, -4959149682788794893,\n",
      "          3980999018101725589,  4569166311984243288,  6099735171050139004,\n",
      "         -3428035850636076912, -2825526036691424401, -8936623199995882903,\n",
      "         -3508914960827407563,  4628985248928164591,  2641293644979187000,\n",
      "          1909703528568070618,  7126066648434563120,  2605128644628282861,\n",
      "         -4877750694853988215,  1979342529003549083],\n",
      "        [   51033500408074872, -7579748500528670082, -6207366836920509646,\n",
      "          7034976646051169721,   103246033564100298,  5549788612483325554,\n",
      "          6756044826896849734,  2595913215783264496,  2988437375558035161,\n",
      "          5450368329466221343,  7300337537563751235,  7568603248944535658,\n",
      "          1041261776262263093,  5997220946845861766,  5184613179297633652,\n",
      "         -7681452786202204266, -4770179032539339334, -5988426183702692342,\n",
      "         -4046347859744051592,  1402648264601908061],\n",
      "        [-3657001648114925196,  6519930155114037478,  5325805802632247210,\n",
      "          8749915503910989606,  2491669274684505720,  1449503563531023908,\n",
      "         -2832002878905207113,  4730600908501105230,  4588024049142848058,\n",
      "          1703178227464989736, -7579153636718442834, -1312680218525856628,\n",
      "         -2823117755046544081, -5532386766558801120, -8826570648578341172,\n",
      "         -4123308294241743828,  3163781996568014006, -7094667924966287193,\n",
      "          7920286712389881656,   263169409391269351],\n",
      "        [-6247647629585507360, -4979698983383357403,  -101309540769824966,\n",
      "         -3298807883287993777,  -371304148899085493,  3631567104612811760,\n",
      "          6600012114804276042, -9095379901328061879,  7012914693473944335,\n",
      "           201026823771738723,  7592643349365263020, -6531662885730518603,\n",
      "         -7983456341774148362, -7115427990884946161,  8936712919372070929,\n",
      "           -90805245076177728,  8781043943490568175, -8218545261846471774,\n",
      "          7089846159916117673,  4213954302970728491],\n",
      "        [-7171004736542205940,  8473697520249552305,  9187835228143405729,\n",
      "           400430172941815369, -7887770033039695995, -1915656817848799934,\n",
      "           146908310488252194,  7494520906810382608, -1502406572351563862,\n",
      "          2886594096551047600, -8709915180506537168, -6873328181645249448,\n",
      "          6687187296247569673, -3607484967720681611,  2701272704715741779,\n",
      "         -4415941236208224475,   665050454100777114, -6741161501742978708,\n",
      "         -5587269491015974627, -4581405193784978632],\n",
      "        [ 7965934322742796706, -4220916451636502832,  2543853615709491490,\n",
      "          3989114663378801255,  -232496029374508498,  1135688758248313567,\n",
      "         -1329084094292610617,  -280967945312878981,  2861159623191970083,\n",
      "          5691348132888905057,  3775684390849415399,  3193156693648416174,\n",
      "         -6890448160150426666, -4502163779660205057, -6219081194101312169,\n",
      "          2915532430416600738, -4871352303923008023,  1583575088075048602,\n",
      "         -6369925783273801360,  2116084725328255381],\n",
      "        [ 8646075179073350508, -6012115500862786555, -7695037160200154696,\n",
      "         -8169556130727286309, -7666593528571449326,  9159409240176047909,\n",
      "          7688955190549263772, -1166807882868010152,  1778844680964439751,\n",
      "           661617157574347008, -6742732957288040753, -2663604104235601729,\n",
      "          2845220725608117275, -4463550834140105905,    87041874796101256,\n",
      "         -4168744363950790024, -5569888896576926053,    85246737548931854,\n",
      "          5117266237363689600, -8523969051953656353],\n",
      "        [ 1417686396845369662,  8578983045271211255,  -325427707970525059,\n",
      "          5230694813940519057, -6619976222734088214,  -574379264012765390,\n",
      "          6725159631059268404, -2764465048554683675,  4202333460695336388,\n",
      "         -6268579866839735443, -3536796191263639747,  2021297733215085205,\n",
      "         -8507233252264556796,  2187773092513908982, -2176817745722599897,\n",
      "         -7706068469076511024, -4263823008837238693, -4986410135498872350,\n",
      "          6056101760276357680, -5436003930669766046],\n",
      "        [-3991849880646401057, -7441600450627229990, -4839827653917358918,\n",
      "         -5943166383402757245, -7882786240734652783, -1252121854881627871,\n",
      "          5016722064556076871,  3238962863953823190, -1787149007161677712,\n",
      "         -7355912605218533965,  3679398210220725882, -6139445604878352712,\n",
      "          6924723336870750633,  1971119922088029609,  8269014641638155405,\n",
      "         -7972167298419495348, -3324927999971199908, -7308135251512412172,\n",
      "          1070791780668894286, -5641814352431756681],\n",
      "        [-4572200648883328376, -3600488752290906335, -1769592825131554514,\n",
      "         -4386580693657226775,  1749755666586969987,  6086011952390348436,\n",
      "          5925174907496730347, -4991174637870559773,  8732162387944222771,\n",
      "          2859002031052160932, -5598014108070866503,  6427556110485138812,\n",
      "         -4208913904164553882,  7806893888526685301,    -2285523721686692,\n",
      "          8633449132370733093,  7227000367975619879,  6004824044585438876,\n",
      "          3243210270428147133,   619193148636563878],\n",
      "        [ 2875152542028292854, -8885451808329035941,   924994353828867113,\n",
      "          7245285682822413332,  1418512364566266627,  7374871067593978799,\n",
      "         -8131774864716529690, -3217175478052853901,  4629565759654734033,\n",
      "          8747260614518677398,   689168540893098381,  8560291934283927311,\n",
      "         -2454514305747704551,  3811493400970093173,  7686644097165378815,\n",
      "          -263585188711338449, -1890040889308811158, -6630606114156873521,\n",
      "          6413544535472717913, -8646862552205645013],\n",
      "        [ -996963643372430706,  7703299726345238179,  5714539980537840573,\n",
      "          2220714484199671071,  2926259514403700021,  1349736466556996631,\n",
      "         -7583123771321188581, -5080718685574424042,  4504842576805640713,\n",
      "         -1507162235352184973, -2186229308719813851, -7843536632408220433,\n",
      "         -9103755107675826632, -7493180691204013265,  4975089939683531482,\n",
      "          1292749358039436155,  8724759943018233029,  4488405512256934172,\n",
      "         -1811143244435947841,  -405932914037007374],\n",
      "        [-8373202287487890994,  9064508624018636657, -7578708979169701205,\n",
      "          7057079406224714039,    73435476824706908, -5613720612687202261,\n",
      "         -5430931466241382113, -6358354282252941888,  2865649734728885893,\n",
      "          1773889645625597936, -8959756410880990336, -6439042711092589414,\n",
      "          2966339476274154855, -7454314673830788641,  1329971158818503031,\n",
      "          7423415730302863863,  -179166948871733312, -5784954529024070558,\n",
      "         -3737935496488439810,  -499974894190213406],\n",
      "        [ 4082756575534186440,  3737601999682012129, -3159240727232312012,\n",
      "         -7760617415440818942,  2114640853025742586,  7386977693739576098,\n",
      "         -8249114747658942935,  7140073809050246058, -9062846008873302861,\n",
      "         -8634457926088319163,  6564996739060989103, -5161695497476095477,\n",
      "         -8166314995771035375, -7582513177752953559, -8264599060712691516,\n",
      "         -5964405964427845477,  2039930743892083196, -6158089241998389953,\n",
      "         -1645547449704524429,  3761992200738141805],\n",
      "        [ 2790422309770674076, -8311206840323596284, -6848380402247476687,\n",
      "         -1006812251806874322, -3999089914536460104,  -779673426918461633,\n",
      "           104931887338773065, -1236605616561819617, -8716185562402127525,\n",
      "         -1533103001517925008,  4428360429247484724,  2702621152843326754,\n",
      "          7401064865401684547,  4245654705027378564,  3153519617659363198,\n",
      "          1390092976192866081,  8944798051231619901,  1577253449072711442,\n",
      "         -7712420828355436517, -8832832220547730510],\n",
      "        [-8405742858797979763, -6801607493843851972, -3910360650174240705,\n",
      "          -505990926354842597,  4751216930170484550, -2022344978451003256,\n",
      "         -1761211961179185032, -7117575952431241963,   281778279973023785,\n",
      "         -9028470159097918493, -1680969166679440902,  -396824213938175342,\n",
      "         -6444917087274618524,  8046880103271054461, -9200487948586340321,\n",
      "          8283974410804585490,  5732789994409103061,  1033521349951961365,\n",
      "          8016634316321621962, -2868623473959629650],\n",
      "        [  957427991646972131,  2004836335198312285,   619748829487064211,\n",
      "          2941856298719975425,  7207820826800354286,  7784286632398756159,\n",
      "         -6874528983167913632, -5832626949372951943, -1877865270204822799,\n",
      "          7586477399159244697, -3148957174196146261, -7262834608156425899,\n",
      "         -7769730457025614008,  3168430209441430547,  3733228283891979567,\n",
      "         -9058527231699275750,  2079775055848031847,  -435025766630368086,\n",
      "         -2877568556224453480,  3022446473639681710],\n",
      "        [-6943007529557340602, -7100220140610475097, -5622993673948617750,\n",
      "          3780290539885292261, -5605291492765786919,  6257183375504868448,\n",
      "           409892757626762726,  4778285622030956072, -1853943783793992562,\n",
      "          -383277353162272792,  4019532551791629662, -8785988959942796801,\n",
      "          5268307178679782960,  9209090163789808848,  9060673827598785883,\n",
      "         -6015832432069760663,    53743464618726306, -4365965156490555252,\n",
      "          -605923772142800731, -6928237112926502540],\n",
      "        [ 8840179004310975144, -1995967236976299585,  -278214651373355795,\n",
      "           643093894623603185, -2546994360070115690,  6879189766231367436,\n",
      "         -1802937763163120755, -5509580998098271941, -2937406582478628108,\n",
      "         -8199432291664746024, -8091093876261156554,  3277859874744140160,\n",
      "         -2185103913797648324,  5547778110676783057,  6267171332406784743,\n",
      "          1004878307630181507, -1581935767638006773,  8443477149161471995,\n",
      "         -8444560365008118473, -8645383103941551462],\n",
      "        [-6122532156654189707, -9095283532684971862, -5785846390907952501,\n",
      "         -7955569284720744862, -5988849580321761515, -6833062242729839476,\n",
      "          2706319606811720936, -7875315535986693331, -1683563042405846665,\n",
      "         -5151397253543595568, -4457984430673418524, -2463439611011361598,\n",
      "           756356335308181673,  1566129713918513649, -2457141219505250339,\n",
      "         -8613420810267071741,  1180970954077035529,  6789648979035663444,\n",
      "          6984296901300687954, -1656564697650062641],\n",
      "        [-4064404292816696101,  -835039007293707731, -7601106679700724707,\n",
      "         -3516459821535220122, -9066518829672436683, -7730510890705993816,\n",
      "          5752473548161902998,   202036300045582073,  7693707815595574819,\n",
      "         -7437365103661769483,  6373177503111632775,  3092009215960546981,\n",
      "         -8366515114254178984,   508009445713910658, -2472486094824501380,\n",
      "         -3560809682654241105,  3035100214719352423,  3629505742322954065,\n",
      "         -8673315723238915033,  8742611339301546769],\n",
      "        [ 7134850171058823792, -8157568061480713221, -4223758466847107498,\n",
      "          5604006296242109449,  6332375037283422330,   696656738104445160,\n",
      "         -4824615466429408530, -7206521930613859039,  -878078628028627360,\n",
      "         -7575182304694526799,  8232497110958941596,  -424238684042401431,\n",
      "         -2784371982104822385, -8952780356503984189, -2702994771668026846,\n",
      "          5015094869198514533,   786333992610489013,  3268688809067024811,\n",
      "         -9052283200101764376, -2122508808419621677],\n",
      "        [ 5381002706292775365,   707586275247377978, -8339842300027928635,\n",
      "          1234232394876935882,  5225148030892220538,  8515429204497869131,\n",
      "         -8436726506705424920, -8165719451269904871, -1463603724583598614,\n",
      "          -198079988243610244,   -19422278717775296,  3822080868959654580,\n",
      "         -1319542375289645054,  4010864646143284895, -8983131970626306480,\n",
      "          6947547644751648470, -8660214074772062256, -7169575259495532702,\n",
      "         -7175507392209298835,  7018772429809184242],\n",
      "        [ 8884833050837214175,  1949070386235454307, -2955796546859994566,\n",
      "         -2265814675548912006,   702287290317080206, -7644550662320022831,\n",
      "         -5218737643842817163,  -348033307790447886,  2023008758327582496,\n",
      "          5538140565880207221, -1095682852601542634,  2531421413834350898,\n",
      "          6151671824398631246, -1404105683758784314,  3118311870615176736,\n",
      "         -3743152077674728604,  1078392297881129309,  8645347360667628862,\n",
      "          5392699730492762533, -5437516266580000373],\n",
      "        [  376326605633887511,  8545374120986750013, -3154429211176621811,\n",
      "         -5802715216584269370,   477799122001872148, -1770771300647761664,\n",
      "         -5507804623285440884, -7820228786090123489,  -903766486585872775,\n",
      "         -8797632728394717177,  4183929870172134212, -5523438981960519133,\n",
      "         -7601130865353182603,  7316522329496595522,  1824475303906620020,\n",
      "          3794124600097058607,  -965801424459086818, -4430307417474190420,\n",
      "         -5742605053296519442, -5808368886514984226],\n",
      "        [-2394206717698148248, -9079154816450895713, -7686097062453665204,\n",
      "          2803620475173850678, -7121048462722535713,  4246058021573671720,\n",
      "          9142238981435464356,  9063605641734691774, -1195323790178008371,\n",
      "          5649232026764145762,  5991722870137613420, -4975286066847515846,\n",
      "         -9206632441638875102,  4718250908301092804, -8676662669041577313,\n",
      "         -4566311693274301679,  2155740476284619154, -1354348339262118242,\n",
      "          4518674962771445470,  -319536546626353674],\n",
      "        [ 5475047707243059969, -7532707314008349579,  6277922398964418193,\n",
      "          6431749217830793624,  7308579197276444025, -1712737664475526370,\n",
      "          8982970104256466186, -6927951289440983515,  5087434783157537747,\n",
      "          1257299883632270226, -3911937387247585638,  1603668561510682747,\n",
      "         -7295424588016635672, -3194922762929949071,  4582925991684959108,\n",
      "         -6014065748853280157,  6984402313457926552, -3274113577101774068,\n",
      "          7231690846837528429,  -592959094690996573],\n",
      "        [ 2548189519701274882,  2779504299078722529,  3274358405103496589,\n",
      "          5478316205378410631,  1214511161750081458, -8262904761898533842,\n",
      "           497386680824193325, -8603135498224549339, -3193884981863183970,\n",
      "         -1545068316016936391,  4675049791052306849,  7928525683059535174,\n",
      "          9116596621534785888, -1751608545374342117,  -323603492560568019,\n",
      "         -1760830661199870711,  8271075323231729975, -6773413714471389916,\n",
      "          8756061253084277508,  6896938723680742719],\n",
      "        [ 6149737682433954051, -6907439395165703568, -3887033591046491183,\n",
      "         -2066810258113007436,  9143452366531350125, -8055236285859936183,\n",
      "         -6979510845582726301, -7273736346297522737, -2992565522553250962,\n",
      "         -7948945688597896697,  6375883769782086388, -5049318446170845060,\n",
      "         -2063699186974927308,  3940488550207288388, -5810805510191796151,\n",
      "          4646131094028939313, -7840417913175882017, -1312946525446271369,\n",
      "         -4316791857949309649, -2956028254705750277],\n",
      "        [ 9134892326225600990, -2824978925210318467,  1505683568170643703,\n",
      "         -8465332735544670751,  6689392041977831089, -7175384500592557195,\n",
      "         -2391940056427738455,  3763711980929463746,  3858412703590700270,\n",
      "          2028726660551092754, -8303394203710559168, -5225000377583641190,\n",
      "         -1110344666656554984,  4571110083930505303,  -195810311890279821,\n",
      "         -9125546541035006959, -8284695523422872762, -1467465416014002139,\n",
      "          4339485663215217533, -5052186224514204304],\n",
      "        [-3385440924056741232, -1327738924829089042, -2501980052449207056,\n",
      "         -7127633348039712834, -5868916595381520452,  4174723263015729587,\n",
      "         -7422631061666908511,  5382459294302602146, -7010966739554723556,\n",
      "         -7453121925895150702, -1255954828680313768,  7706131853270747117,\n",
      "         -5658166379153952929,  1997115832696530812,  6414914588430209202,\n",
      "         -5685795017961442510, -7568407182947554284, -4766975264793061518,\n",
      "          4783213200056838235,  5932012444352098560],\n",
      "        [-3242051076432596267,  5595944546383850106,  7565985936179039300,\n",
      "          1668089270941418674,  6131603690155083289,  1853428563004829524,\n",
      "          7551131888688815491,  5471646289223674574, -1316551968675706212,\n",
      "         -3069619378839838990,  3293495506111513707,  6073706232922909884,\n",
      "         -3395340910594543241, -4581174716382178682, -9185905767816483949,\n",
      "         -1732640561697545335,   417448532745988073,  7996009747162734026,\n",
      "         -8972789571254189561, -5826688315645655254],\n",
      "        [-6834182050327496634, -6091922599652498959, -2937459750769924812,\n",
      "          1370772276913571817, -6291202772558997515, -5213087303688826754,\n",
      "          9085294118474960865, -6125231537425462639,  4843658785707520279,\n",
      "          6765554540341399087,  2192563368037474269,  7185719158167518031,\n",
      "          2231502190536833015,  8061309003049236167, -2204776993905287850,\n",
      "         -6673170425490981955,  1370053508556220426, -1230237934581568660,\n",
      "         -8737176663885250705,  1497784470140826511],\n",
      "        [-1192384616141068418,  -195433013324276752,  1259178853300939198,\n",
      "         -7828948346105100420,  6793278115758045117,  6317041225311923993,\n",
      "          2787751892118743298, -1111333503583377010,  7127542217523150412,\n",
      "         -3853227055193737220,  4436266946156353643,  7583358428569138283,\n",
      "         -3532150444129498235,  6976365497880780806, -7463008958984153583,\n",
      "          5827742420590776972,  2142037726066175121, -5476377932498469745,\n",
      "         -3336220864220766975, -2888458212027212993],\n",
      "        [ 2259733146140543664, -4196501058340299200,  4074224620323261229,\n",
      "           980274962269403610,  3670751964142460730, -9211405537583002115,\n",
      "         -1346055695108566319,   109129218784026915,  4486719120822271172,\n",
      "          7774343746051375116, -1454146247440536397, -4404318275214470507,\n",
      "           383292964189598382,  7030937772563154895, -5925335668100040119,\n",
      "         -4233598085829543014, -8086511002291741555,  6124468658515174591,\n",
      "          1986476220081850888,   191566803917556959],\n",
      "        [   43375543615700505, -4200690427526069537, -8386075656674441860,\n",
      "           420830846036174038, -5906414232757539203,  7619034089374787172,\n",
      "         -1210016022621560533,  3644056818182519511,   190325748217553271,\n",
      "           804912791247039551,  1337071808029927904,  8747535359291561969,\n",
      "          8589767585518043094,  7346920963626421650,  5443287822950723685,\n",
      "          -904637167619612203, -1945263472381322634, -7872801629213167956,\n",
      "          8326662702027151293,  2779431963200904309],\n",
      "        [-7652961635074033221,  4482382067448652725,  7185759163404839539,\n",
      "          7227806497273656267, -2792274773847383311, -1976589780351065464,\n",
      "         -7823352225593571827, -2466225235101968612, -5542003671167599887,\n",
      "         -7907130384111429572,  6817309670255739546, -2185224169227886145,\n",
      "          1774492960428896927, -5017977051184393055,  3776572588087355321,\n",
      "          2182808959593835018,   643126301715960374, -1237763226861112515,\n",
      "          8213193799290960223,  6146696629987815946],\n",
      "        [ 2503964471394304490,  5366743791212773390, -6895354419141879386,\n",
      "          8460985143556393784,  7105406182887116551, -3345204583401629527,\n",
      "         -1959326056148281751,  -235499204522370132,  5060316032347404437,\n",
      "          4053445742262480195, -7630679360665414473, -8812582305428991055,\n",
      "         -9033890982854018804,   311697853617482039, -7021126400888861447,\n",
      "         -1497037821395806626,  4537480507211146908,  1472668446989065293,\n",
      "         -6221329463149321621, -6700941123548582788],\n",
      "        [ -383843921234953039,  1392754746213469002, -4950921809411660648,\n",
      "         -5521542365231692308, -7439297545151756473,  4386759413396186848,\n",
      "           388257143537707977,  1176920536610990049,  -633612740462919587,\n",
      "          4688753753057282290, -1603019259992894329,  5165931586336875509,\n",
      "          1601613240631935849,  2679344237019511052,  5029402664230856471,\n",
      "         -3203819764214642396,  4970187255762752794, -6543041436630327455,\n",
      "          6877207075301870301,  4914494680125964711],\n",
      "        [-4769330917262717789,  1765497962469940847,  1799322509984109512,\n",
      "         -1108335122236453461,  -299766377060083652, -4641572140556323314,\n",
      "          3928647915232540810,  5173287117484611740,  7797650945264395799,\n",
      "         -5590200814467691413,  6986557303945838014,  6352647469831313030,\n",
      "          -799016832463467026,  -401393649303156061,  4485817280349726025,\n",
      "         -8469596769588793719, -1747919027053051073,  7241088400568173119,\n",
      "          5321023474564122249, -5738666012159180378],\n",
      "        [-7908276788999203412, -7328308325909278655,   190534744369131023,\n",
      "          2120030297683965238, -5730827441996852575, -1977773848953753568,\n",
      "          9141789702553695506,  2849260551220155515,  7265251627578019412,\n",
      "         -1886897381886717254, -4591512681834872704, -6412032495129622901,\n",
      "         -4043929904671850334,  7537257234267387129,  4032220343739470750,\n",
      "          4494424059908891862, -3836350598664290563, -7037366800826839846,\n",
      "         -8498186590734531230,  5910640029348852290],\n",
      "        [-1898899788146386559, -2281415385845842958,  6192680664898528544,\n",
      "         -2848875191463171213, -3848260571886576273,  4710081289300611825,\n",
      "         -1009539374625093882,  7869473932122620481,  -230203422097541236,\n",
      "         -1622301583734054442,  8596743280346794623, -2391582736141362111,\n",
      "         -1093028290388513972,  6427721011092631227, -1154654285088646073,\n",
      "          5183061710791681958, -5487999871965549323,  -393871104732090811,\n",
      "          4678906034073112653,  8643645450827406916],\n",
      "        [-6175940445762136515,  1632709014389993499,  8895485468703857755,\n",
      "          -169578649487421798, -3883427907408508125, -5654348740159223493,\n",
      "           502430761244875583, -3190135804215407987,   180242306794812689,\n",
      "          4506995124208726447,  2557952700053038907,  7046247172164735987,\n",
      "           189949406237536274,  1548268109707036916,  3981457525660404621,\n",
      "          1611711005157656799, -2286872762921425626,   200605057759993666,\n",
      "          2398082627190573155,  6639136739911877016],\n",
      "        [-7051247254792417784, -8150324123449888338,  8742458676233559312,\n",
      "         -6193088797000166971, -2757619578482676002, -3782957945793499738,\n",
      "          8086598196172330716,   479642860904147596, -1105748029894723066,\n",
      "          6692462672342221458, -2902463690111156000,  2486726790760743505,\n",
      "         -5573765846390143900, -5366387652425222360, -7499960882292415428,\n",
      "         -6832890653238881261, -2173667483515888757, -2173518358388260107,\n",
      "         -1866406869128678677, -4109086349522556410],\n",
      "        [-8052802894859298076,  8680892794167848921,  8949885447346370029,\n",
      "         -3321606721399223238,  9126089386389556423,   494819621869962756,\n",
      "          3634987423224840271,  3542235320615663358, -5880546709688967789,\n",
      "          5719753414260745312, -7334033505267307313,  8171847960891397449,\n",
      "         -6620153564240181469, -9106826080714640854,  7760022458795296532,\n",
      "         -8317421324084867644,  4897794570072981188,  3444131106716848083,\n",
      "         -6719107595420290009,  8060867267452067794],\n",
      "        [ 8341150333456785053,   485667059363914526,  6711196348661172576,\n",
      "         -5330331620885799243, -6431439230353317653,  5821688665117185585,\n",
      "         -4117028686283181999,  7844721201449338755,  2912787967240489544,\n",
      "           298851223056250030,  6170173265738911320, -2086041381460810187,\n",
      "          4628066104944392979, -9160695375060861007,  8730098563056874786,\n",
      "          7318130137456934956,  3478439040528747340, -6107284621751009433,\n",
      "          3854981367652500885,  4432057281187681803],\n",
      "        [-5769459013034082539,  8780019039463523460,  3205511263429184923,\n",
      "          6660070787558169792,  5582000366295522692, -2458044319871737664,\n",
      "           931605516809444427, -1926240395414140464, -3394559099647904051,\n",
      "         -6695010663333829278,  4945726213062059117,  8354427615920667715,\n",
      "         -5400358943946827764, -4941774936346625699, -4750055334973372022,\n",
      "          8012287759178330606,  1243620145552454002,  -778341804692596071,\n",
      "         -4708302134723315044,  1585649946527618200],\n",
      "        [-5952475353189378753, -1325472467235377496, -6306211660091562489,\n",
      "         -6276316190335630544, -1087867177613136288,  1887450314217259635,\n",
      "           589585565603098086, -2156767586574136935, -6028560678323799667,\n",
      "         -6776360141017588038,  4824751037029409935, -2461904886614951034,\n",
      "          2691075456992480227,   269269076707832670, -3411941775764845845,\n",
      "          5641973149079091083,  5093593549792555543,  -732066163530573874,\n",
      "          9011288301568032368, -4005448716457613699],\n",
      "        [ 5875821436152527942, -4540539976653508159, -6711194299840469714,\n",
      "         -4340249179838928671,  2970866482920955949,  -562186443403799142,\n",
      "          4591823827000541321,  9038991139374720879, -3096108488627373647,\n",
      "          2049471177413905766, -3411333509251270047,  7588303354016821059,\n",
      "         -2156487147257907808,   943967810689412303, -3634617919587860240,\n",
      "         -4499795848986619232,   211450814595082834, -6176802610071210454,\n",
      "          2891174351740793266,  7655379638625209976],\n",
      "        [-2872157386517033056, -9005578491756535730,  4529939714124830716,\n",
      "          2273998201736675298, -8615246842344412104, -6632230416440556433,\n",
      "          7818173164028846494,  8569730384212174513,  6668510849803524658,\n",
      "         -1371037162608497926, -3422938739454612543, -1565440657774796412,\n",
      "          -444126252663830875, -1628369401833144052, -2508725965669941255,\n",
      "          2201894812335453804,  3419748837528262714,  1092351858843830709,\n",
      "          3124776907674587053, -7763776655640006078]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ 5788404911118579348, -2709605788077721786,  5222074858532603818,\n",
      "         -8204345877110044338, -1669139034820717292,  -613263929638628594,\n",
      "          -918839918368123194,  -880776637130631394,  5593079430486534069,\n",
      "          1989490970514920596,  8842948109518683196, -6898074462858775975,\n",
      "          5052718947528008624,  1675918595413766890,  7397162259254767117,\n",
      "          6713842151865608749, -7945333323881867770,  5385959868078931812,\n",
      "         -9155155247350006888, -7969670597267823974],\n",
      "        [ 5788322923822061970, -2709559682001179559,  5222188129158287492,\n",
      "         -8204121040875005864, -1669238381269877569,  -613316561384269054,\n",
      "          -918913727374578519,  -880688827343165050,  5593053160412997012,\n",
      "          1989483145401791094,  8842963175932556955, -6898046649564195784,\n",
      "          5052834395541007444,  1676088331139488867,  7397258363654846265,\n",
      "          6713738564773243397, -7945203676203486629,  5386027024255195745,\n",
      "         -9155012017875308087, -7969707454234047720],\n",
      "        [ 5788336084581312714, -2709744184559890755,  5222253075026031021,\n",
      "         -8204347784752858035, -1668992484044540773,  -613166393202391675,\n",
      "          -918760756380425065,  -880675286727085488,  5593089365329961567,\n",
      "          1989590746979419207,  8843043995882881306, -6898208636054530254,\n",
      "          5052670670898856630,  1676144048263835809,  7397250292388741217,\n",
      "          6713717186892597180, -7945146030133510897,  5385955982925726314,\n",
      "         -9155111332705631671, -7969655430041374545]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")MPCTensor(\n",
      "\t_tensor=tensor([[-5788404911118580675,  2709605788077717220, -5222074858532608735,\n",
      "          8204345877110062947,  1669139034820733881,   613263929638581905,\n",
      "           918839918368133594,   880776637130653908, -5593079430486510419,\n",
      "         -1989490970514916769, -8842948109518672980,  6898074462858787976,\n",
      "         -5052718947527910889, -1675918595413814623, -7397162259254689276,\n",
      "         -6713842151865589344,  7945333323881910497, -5385959868078936246,\n",
      "          9155155247349941609,  7969670597267807326],\n",
      "        [-5788322923821993669,  2709559682001130421, -5222188129158212297,\n",
      "          8204121040875010150,  1669238381269888972,   613316561384215895,\n",
      "           918913727374613229,   880688827343240675, -5593053160412983063,\n",
      "         -1989483145401858866, -8842963175932597749,  6898046649564234947,\n",
      "         -5052834395541091388, -1676088331139494817, -7397258363654778025,\n",
      "         -6713738564773278212,  7945203676203502178, -5386027024255227861,\n",
      "          9155012017875281726,  7969707454234164900],\n",
      "        [-5788336084581314877,  2709744184559930187, -5222253075025878880,\n",
      "          8204347784752947357,  1668992484044518219,   613166393202386630,\n",
      "           918760756380399604,   880675286727169481, -5593089365329977561,\n",
      "         -1989590746979343828, -8843043995882797166,  6898208636054503331,\n",
      "         -5052670670898958827, -1676144048263879446, -7397250292388844007,\n",
      "         -6713717186892687649,  7945146030133411018, -5385955982925733478,\n",
      "          9155111332705694282,  7969655430041354837]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n",
      "\n",
      "Rank: 0\n",
      "\tFirst Linear Layer: Output Encrypted: True\n",
      "Rank: 1\n",
      "\tFirst Linear Layer: Output Encrypted: True\n",
      "Rank: 0\n",
      "\tShares after First Linear Layer:tensor([[-5788404911118580675,  2709605788077717220, -5222074858532608735,\n",
      "          8204345877110062947,  1669139034820733881,   613263929638581905,\n",
      "           918839918368133594,   880776637130653908, -5593079430486510419,\n",
      "         -1989490970514916769, -8842948109518672980,  6898074462858787976,\n",
      "         -5052718947527910889, -1675918595413814623, -7397162259254689276,\n",
      "         -6713842151865589344,  7945333323881910497, -5385959868078936246,\n",
      "          9155155247349941609,  7969670597267807326],\n",
      "        [-5788322923821993669,  2709559682001130421, -5222188129158212297,\n",
      "          8204121040875010150,  1669238381269888972,   613316561384215895,\n",
      "           918913727374613229,   880688827343240675, -5593053160412983063,\n",
      "         -1989483145401858866, -8842963175932597749,  6898046649564234947,\n",
      "         -5052834395541091388, -1676088331139494817, -7397258363654778025,\n",
      "         -6713738564773278212,  7945203676203502178, -5386027024255227861,\n",
      "          9155012017875281726,  7969707454234164900],\n",
      "        [-5788336084581314877,  2709744184559930187, -5222253075025878880,\n",
      "          8204347784752947357,  1668992484044518219,   613166393202386630,\n",
      "           918760756380399604,   880675286727169481, -5593089365329977561,\n",
      "         -1989590746979343828, -8843043995882797166,  6898208636054503331,\n",
      "         -5052670670898958827, -1676144048263879446, -7397250292388844007,\n",
      "         -6713717186892687649,  7945146030133411018, -5385955982925733478,\n",
      "          9155111332705694282,  7969655430041354837]])\n",
      "Rank: 1\n",
      "\tShares after First Linear Layer:tensor([[ 5788404911118579348, -2709605788077721786,  5222074858532603818,\n",
      "         -8204345877110044338, -1669139034820717292,  -613263929638628594,\n",
      "          -918839918368123194,  -880776637130631394,  5593079430486534069,\n",
      "          1989490970514920596,  8842948109518683196, -6898074462858775975,\n",
      "          5052718947528008624,  1675918595413766890,  7397162259254767117,\n",
      "          6713842151865608749, -7945333323881867770,  5385959868078931812,\n",
      "         -9155155247350006888, -7969670597267823974],\n",
      "        [ 5788322923822061970, -2709559682001179559,  5222188129158287492,\n",
      "         -8204121040875005864, -1669238381269877569,  -613316561384269054,\n",
      "          -918913727374578519,  -880688827343165050,  5593053160412997012,\n",
      "          1989483145401791094,  8842963175932556955, -6898046649564195784,\n",
      "          5052834395541007444,  1676088331139488867,  7397258363654846265,\n",
      "          6713738564773243397, -7945203676203486629,  5386027024255195745,\n",
      "         -9155012017875308087, -7969707454234047720],\n",
      "        [ 5788336084581312714, -2709744184559890755,  5222253075026031021,\n",
      "         -8204347784752858035, -1668992484044540773,  -613166393202391675,\n",
      "          -918760756380425065,  -880675286727085488,  5593089365329961567,\n",
      "          1989590746979419207,  8843043995882881306, -6898208636054530254,\n",
      "          5052670670898856630,  1676144048263835809,  7397250292388741217,\n",
      "          6713717186892597180, -7945146030133510897,  5385955982925726314,\n",
      "         -9155111332705631671, -7969655430041374545]])\n",
      "Get attributeGet attribute  forwardforward\n",
      "\n",
      "MPCTensor(\n",
      "\t_tensor=tensor([[ -68759711539011,   70168005525579,   13214471180616,  -96079628196411,\n",
      "           34415907676402,  -72644163898831, -131970628836721, -119035386150096,\n",
      "           19841428952753,  -81550787400292,   82027903861765,  124307338143115,\n",
      "          -98223738745859,   41912355231684,  -71136020205159,  -29778356231307,\n",
      "           33409982186654,  -72159530872250,   23839085112608,   45302499068605],\n",
      "        [  56197999232865,  -19380742994759,  129826183818767,   57818681014477,\n",
      "          131093569687658,  -81623533897259,   97085575571401,  -98481744874361,\n",
      "         -107632414675625,  -93499017660616,  -65300655285439,  -27444246387759,\n",
      "         -137420134637529,  -55369169138123, -114960606451376,  140492134181047,\n",
      "          -80030679253658,  -27598165782683,   55541836181890,   37885308403960],\n",
      "        [  25978514330282,   74689825949803,   71005229359810,  -63322608040678,\n",
      "          101446246317303,   12691121460303,   43332003094166, -126876567155811,\n",
      "          -66075978747837, -139872259360830, -129459150720552,  -66736216764634,\n",
      "         -127696340957824,  -24848107576369,   21547255653623,  -17445501226411,\n",
      "          -71848610730560,  -80980845573623,   77710958495086,  139088189060830]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")MPCTensor(\n",
      "\t_tensor=tensor([[  68759711539011,  -70168005525579,  -13214471180616,   96079628215020,\n",
      "          -34415907659813,   72644163898831,  131970628847121,  119035386172610,\n",
      "          -19841428929103,   81550787404119,  -82027903851549, -124307338131114,\n",
      "           98223738843594,  -41912355231684,   71136020283000,   29778356250712,\n",
      "          -33409982143927,   72159530872250,  -23839085112608,  -45302499068605],\n",
      "        [ -56197999164564,   19380742994759, -129826183743572,  -57818681010191,\n",
      "         -131093569676255,   81623533897259,  -97085575536691,   98481744949986,\n",
      "          107632414689574,   93499017660616,   65300655285439,   27444246426922,\n",
      "          137420134637529,   55369169138123,  114960606519616, -140492134181047,\n",
      "           80030679269207,   27598165782683,  -55541836181890,  -37885308286780],\n",
      "        [ -25978514330282,  -74689825910371,  -71005229207669,   63322608130000,\n",
      "         -101446246317303,  -12691121460303,  -43332003094166,  126876567239804,\n",
      "           66075978747837,  139872259436209,  129459150804692,   66736216764634,\n",
      "          127696340957824,   24848107576369,  -21547255653623,   17445501226411,\n",
      "           71848610730560,   80980845573623,  -77710958432475, -139088189060830]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "\n",
      "Rank: 0\n",
      "\tReLU:\n",
      " Output Encrypted: True\n",
      "Rank: 1\n",
      "\tReLU:\n",
      " Output Encrypted: True\n",
      "Rank: 0\n",
      "\tShares after ReLU: tensor([[ -68759711539011,   70168005525579,   13214471180616,  -96079628196411,\n",
      "           34415907676402,  -72644163898831, -131970628836721, -119035386150096,\n",
      "           19841428952753,  -81550787400292,   82027903861765,  124307338143115,\n",
      "          -98223738745859,   41912355231684,  -71136020205159,  -29778356231307,\n",
      "           33409982186654,  -72159530872250,   23839085112608,   45302499068605],\n",
      "        [  56197999232865,  -19380742994759,  129826183818767,   57818681014477,\n",
      "          131093569687658,  -81623533897259,   97085575571401,  -98481744874361,\n",
      "         -107632414675625,  -93499017660616,  -65300655285439,  -27444246387759,\n",
      "         -137420134637529,  -55369169138123, -114960606451376,  140492134181047,\n",
      "          -80030679253658,  -27598165782683,   55541836181890,   37885308403960],\n",
      "        [  25978514330282,   74689825949803,   71005229359810,  -63322608040678,\n",
      "          101446246317303,   12691121460303,   43332003094166, -126876567155811,\n",
      "          -66075978747837, -139872259360830, -129459150720552,  -66736216764634,\n",
      "         -127696340957824,  -24848107576369,   21547255653623,  -17445501226411,\n",
      "          -71848610730560,  -80980845573623,   77710958495086,  139088189060830]])\n",
      "\n",
      "Rank: 1\n",
      "\tShares after ReLU: tensor([[  68759711539011,  -70168005525579,  -13214471180616,   96079628215020,\n",
      "          -34415907659813,   72644163898831,  131970628847121,  119035386172610,\n",
      "          -19841428929103,   81550787404119,  -82027903851549, -124307338131114,\n",
      "           98223738843594,  -41912355231684,   71136020283000,   29778356250712,\n",
      "          -33409982143927,   72159530872250,  -23839085112608,  -45302499068605],\n",
      "        [ -56197999164564,   19380742994759, -129826183743572,  -57818681010191,\n",
      "         -131093569676255,   81623533897259,  -97085575536691,   98481744949986,\n",
      "          107632414689574,   93499017660616,   65300655285439,   27444246426922,\n",
      "          137420134637529,   55369169138123,  114960606519616, -140492134181047,\n",
      "           80030679269207,   27598165782683,  -55541836181890,  -37885308286780],\n",
      "        [ -25978514330282,  -74689825910371,  -71005229207669,   63322608130000,\n",
      "         -101446246317303,  -12691121460303,  -43332003094166,  126876567239804,\n",
      "           66075978747837,  139872259436209,  129459150804692,   66736216764634,\n",
      "          127696340957824,   24848107576369,  -21547255653623,   17445501226411,\n",
      "           71848610730560,   80980845573623,  -77710958432475, -139088189060830]])\n",
      "\n",
      "Get attribute"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def step_through_two_layers():    \n",
    "    rank = comm.get().get_rank()\n",
    "\n",
    "    # Load and encrypt the network\n",
    "    model = crypten.load_from_party(sample_trained_model_file, model_class=AliceNet, src=ALICE)\n",
    "    private_model = crypten.nn.from_pytorch(model, dummy_input=torch.empty((1, 50)))\n",
    "    private_model.encrypt(src=ALICE)\n",
    "\n",
    "    # Load and encrypt the data\n",
    "    data_enc = crypten.load_from_party(sample_data_bob_file, src=BOB)\n",
    "\n",
    "    # Forward through the first layer\n",
    "    out_enc = private_model._modules['5'].forward(data_enc)\n",
    "    encrypted = crypten.is_encrypted_tensor(out_enc)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tFirst Linear Layer: Output Encrypted: {encrypted}\", in_order=True)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tShares after First Linear Layer:{out_enc.share}\", in_order=True)\n",
    "\n",
    "    # Apply ReLU activation\n",
    "    out_enc = private_model._modules['6'].forward(out_enc)\n",
    "    encrypted = crypten.is_encrypted_tensor(out_enc)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tReLU:\\n Output Encrypted: {encrypted}\", in_order=True)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tShares after ReLU: {out_enc.share}\\n\", in_order=True)\n",
    "\n",
    "    # Forward through the second Linear layer\n",
    "    out_enc = private_model._modules['output'].forward(out_enc)\n",
    "    encrypted = crypten.is_encrypted_tensor(out_enc)\n",
    "    crypten.print(f\"Rank: {rank} Second Linear layer:\\n Output Encrypted: {encrypted}\\n\", in_order=True) \n",
    "    crypten.print(f\"Rank: {rank} Shares after Second Linear layer:{out_enc.share}\\n\", in_order=True)\n",
    "\n",
    "    # Decrypt the output\n",
    "    out_dec = out_enc.get_plain_text()\n",
    "    \n",
    "    # Since both parties have same decrypted results, only print the rank 0 output\n",
    "    crypten.print(\"Decrypted output:\\n Output Encrypted:\", crypten.is_encrypted_tensor(out_dec))\n",
    "    crypten.print(\"Tensors:\\n\", out_dec)\n",
    "    \n",
    "z = step_through_two_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we emphasize that the output of each layer is an encrypted tensor. Only after the final call to `get_plain_text` do we get the plaintext tensor.\n",
    "\n",
    "### From PyTorch to CrypTen: Structural Changes in Network Architecture \n",
    "\n",
    "We have used a simple two-layer network in the above example, but the same ideas apply to more complex networks and operations. However, in more complex networks, there may not always be a one-to-one mapping between the PyTorch layers and the CrypTen layers. This is because we use PyTorch's onnx implementation to convert PyTorch models to CrypTen models. \n",
    "As an example, we'll take a typical network used to classify digits in MNIST data, and look at what happens to its structure we convert it to a CrypTen module. (As we only wish to illustrate the structural changes in layers, we will not train this network on data; we will just use it with its randomly initialized weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Alice's network\n",
    "class AliceNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(100)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.batchnorm3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = AliceNet2()\n",
    "\n",
    "# Let's encrypt the complex network. \n",
    "# Create dummy input of the correct input shape for the model\n",
    "dummy_input = torch.empty((1, 1, 28, 28))\n",
    "\n",
    "# Encrypt the network\n",
    "private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "private_model.encrypt(src=ALICE)\n",
    "\n",
    "# Examine the structure of the encrypted network\n",
    "for name, curr_module in private_model._modules.items():\n",
    "    print(\"Name:\", name, \"\\tModule:\", curr_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the CrypTen network has split some the layers in the PyTorch module into several CrypTen modules. Each PyTorch operation may correspond to one or more operations in CrypTen. However, during the conversion, these are sometimes split due to limitations intorduced by onnx.\n",
    "\n",
    "Before exiting this tutorial, please clean up the files generated using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for fn in temp_files:\n",
    "    if os.path.exists(fn): os.remove(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
