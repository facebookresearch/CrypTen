{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification with Encrypted Neural Networks\n",
    "\n",
    "In this tutorial, we'll look at how we can achieve the <i>Model Hiding</i> application we discussed in the Introduction. That is, let's say Alice has a trained model she wishes to keep private, and Bob has some data he wishes to classify while keeping it private. We'll see how CrypTen allows Alice and Bob to coordinate and classify the data, while achieving their privacy requirements.\n",
    "\n",
    "To simulate this scenario, we'll begin with Alice training a simple neural network on MNIST data. Then we'll see how Alice and Bob encrypt their network and data respectively, classify the encrypted data and finally decrypt the labels.\n",
    "\n",
    "## Initialization\n",
    "Let's load some MNIST data, and train Alice's network on it. We first import the `torch` and `crypten` libraries, and initialize `crypten`. We will use a helper script `mnist_utils.py` to split the public MNIST data into Alice's portion and Bob's portion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "\n",
    "crypten.init()\n",
    "\n",
    "#ignore warnings\n",
    "import warnings; \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script that downloads the publicly available MNIST data, and splits the data as required.\n",
    "%run ./mnist_utils.py --option train_v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.3391\n",
      "\tAccuracy: 92.7083\n",
      "Epoch 1 Loss: 0.2689\n",
      "\tAccuracy: 96.8750\n"
     ]
    }
   ],
   "source": [
    "# Alice creates and trains her network on her data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define Alice's network\n",
    "class AliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "model = AliceNet()\n",
    "\n",
    "# Load Alice's data\n",
    "data_alice = crypten.load('/tmp/alice_train.pth', src=0)\n",
    "label_alice = crypten.load('/tmp/alice_train_labels.pth', src=0)\n",
    "data_alice_flat = data_alice.flatten(start_dim=1)\n",
    "\n",
    "# Train Alice's network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6)\n",
    "\n",
    "num_examples = 60000\n",
    "batch_size = 256\n",
    "num_epochs = 2\n",
    "log_accuracy = True\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for j in range(0, num_examples, batch_size):\n",
    "        \n",
    "        # get the mini-batch\n",
    "        start, end = j, min(j+batch_size,num_examples)\n",
    "        sample_flat = data_alice_flat[start:end,:]\n",
    "        target = label_alice[start:end]\n",
    "        \n",
    "        # forward pass: compute prediction\n",
    "        output = model(sample_flat)\n",
    "\n",
    "        # compute and print loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # zero gradients for learnable parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward pass: compute gradient with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    # log accuracy every epoch\n",
    "    if log_accuracy:\n",
    "        pred = output.argmax(1)\n",
    "        correct = pred.eq(target)\n",
    "        correct_count = correct.sum(0, keepdim=True).float()\n",
    "        accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "        print(\"Epoch {0} Loss: {1:.4f}\".format(i, loss.item()))\n",
    "        print(\"\\tAccuracy: {0:.4f}\".format(accuracy.item()))\n",
    "        \n",
    "torch.save(model, '/tmp/alice_model.ptr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encryption\n",
    "Alice now has a trained neural network that can classify data. Let's see how we can use CrypTen to encrypt this network, so it can be used to classify data without revealing its parameters. \n",
    "\n",
    "In CrypTen, encrypting PyTorch network is straightforward: first, we call the function `from_pytorch` that sets up a CrypTen network from the PyTorch network. Then, we call `encrypt` on the CrypTen network to encrypt its parameters. After encryption, the CrypTen network can also decrypted (see the `decrypt` function).\n",
    "\n",
    "In addition to the PyTorch network, the `from_pytorch` function also requires a dummy input of the shape of the model's input. The dummy input simply needs to be a `torch` tensor of the same shape; the values inside the tensor do not matter. (This is a requirement of `torch.distributed`, our communication backend.) After calling the `encrypt` function, we'll be able to check that the model is encrypted. (We won't actually look into how the model is encrypted in this tutorial; we'll leave that to Tutorial 5). \n",
    "\n",
    "We'll also encrypt Bob's data -- this step is identical to what we've seen in Tutorial 3. Let's walk through an example below.\n",
    "\n",
    "<i><small>(Technical note: Since Jupyter notebooks only run a single process, we use a custom decorator `mpc.run_multiprocess` to simulate a multi-party world below.)</small><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption flag in CrypTen model: True\n",
      "Encryption of Data: True\n"
     ]
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def encrypt_model_and_data():\n",
    "    \n",
    "    rank = comm.get().get_rank()\n",
    "    \n",
    "    if rank == 0:\n",
    "        #Alice gets the trained model\n",
    "        plaintext_model = model\n",
    "    else:\n",
    "        #Bob gets a dummy model with random parameters\n",
    "        plaintext_model = AliceNet()\n",
    "\n",
    "    #Encrypt the model    \n",
    "    #Create a dummy input with the same shape as the model input\n",
    "    dummy_input = torch.empty((1, 784))\n",
    "    #Construct a CrypTen network with the trained model and dummy_input\n",
    "    private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "    #Encrypt the CrypTen network. Alice has the real model, so we encrypt with src=0\n",
    "    private_model.encrypt(src=0)\n",
    "    #The model is now encrypted: we can check the model's 'encrypted' flag!\n",
    "    if rank == 0:\n",
    "        print(\"Encryption flag in CrypTen model:\", private_model.encrypted)\n",
    "    \n",
    "    plaintext_data = crypten.load('/tmp/bob_test.pth', src=1)\n",
    "        \n",
    "    #Encrypt the data\n",
    "    #Bob has the real data, so we encrypt with src=1\n",
    "    data_enc = crypten.cryptensor(plaintext_data, src=1)\n",
    "    if rank == 0:\n",
    "        print(\"Encryption of Data:\", crypten.is_encrypted_tensor(data_enc))\n",
    "    #print(rank, data_enc._tensor, \"\\n\")\n",
    "        \n",
    "z = encrypt_model_and_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Encrypted Data with Encrypted Model\n",
    "We can finally use Alice's encrypted network to classify Bob's encrypted data. This step is identical to PyTorch, except we'll use the encrypted network and data instead of the plaintext versions that PyTorch uses. \n",
    "\n",
    "<small><i>(Technical note: We have simulated a multi-party world with the `@mpc.run_multiprocess` decorator in the previous cell. However, as a result, the variables loaded do not carry over from cell to cell as is customary in a notebook. Therefore, in order to illustrate next steps, we reinitialize  `model_enc` and `data_enc` both with `src=0`)</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is required for demonstrating the learning algorithm in our notebook. \n",
    "# As Jupyter notebooks run only a single process, the model and the data both need to be encrypted\n",
    "# with src=0 in order for the remaining code to run. In a regular CrypTen implementation \n",
    "# (see the CrypTen examples folder), data_enc would be encrypted with src=1 as shown in the cell above.\n",
    "dummy_input = torch.empty((1, 784))\n",
    "private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "private_model.encrypt(src=0)\n",
    "\n",
    "#These next two lines would use src=1 outside Jupyter notebooks\n",
    "plaintext_data = crypten.load('/tmp/bob_test.pth', src=0)\n",
    "data_enc = crypten.cryptensor(plaintext_data, src=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run inference on the encrypted network with the encrypted data\n",
    "private_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "     output_enc = private_model(data_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this classification is encrypted. To see this, here let's just check whether the result is an encrypted tensor; in the next tutorial, we'll look into the values of tensor and confirm the encryption. \n",
    "\n",
    "We can now decrypt the result. As we discussed before, Alice and Bob both have access to the decrypted output of the model, and can both use this to obtain the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor encrypted: True\n",
      "Decrypted output:\n",
      " tensor([[ 0.7568, -5.5644,  4.4242,  ..., 10.5856, -0.4696,  2.2868],\n",
      "        [ 1.3657, -0.2141,  8.0961,  ..., -7.2969,  2.1092, -6.8442],\n",
      "        [-5.6699,  6.8439,  0.7923,  ...,  0.3638,  0.1249, -1.8369],\n",
      "        ...,\n",
      "        [-5.8757, -5.8690, -2.3751,  ...,  0.8419,  3.6848,  3.6178],\n",
      "        [-1.6579, -0.4770, -3.1302,  ..., -3.3471,  4.1610, -2.9383],\n",
      "        [ 2.2382, -4.9993,  3.6461,  ..., -7.7665, -1.0910, -4.1141]])\n",
      "Decrypted labels:\n",
      " tensor([7, 2, 1,  ..., 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "#The results are encrypted: \n",
    "print(\"Output tensor encrypted:\", crypten.is_encrypted_tensor(output_enc)) \n",
    "\n",
    "#Decrypting the result\n",
    "output = output_enc.get_plain_text()\n",
    "print(\"Decrypted output:\\n\", output)\n",
    "\n",
    "#Obtaining the labels\n",
    "pred = output.argmax(dim=1)\n",
    "print(\"Decrypted labels:\\n\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 94.3300\n"
     ]
    }
   ],
   "source": [
    "#Finally, we'll compute the accuracy of the output:\n",
    "output = output_enc.get_plain_text()\n",
    "label_bob = crypten.load('/tmp/bob_test_labels.pth')\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = output.argmax(1)\n",
    "    correct = pred.eq(label_bob)\n",
    "    correct_count = correct.sum(0, keepdim=True).float()\n",
    "    accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "    print(\"Accuracy {0:.4f}\".format(accuracy.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This completes our tutorial. While we have used a simple network here to illustrate the concepts, CrypTen provides primitives to allow for encryption of substantially more complex networks. In our examples section, we demonstrate how CrypTen can be used to encrypt LeNet and ResNet, among others."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "390894444956881"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139932",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "375902760006757",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 4 -- Classification with Encrypted Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
